{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "20210901.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fe7bfb5",
        "outputId": "af5893ee-f09a-4ca1-9959-85b629026bf9"
      },
      "source": [
        "pip install transformers"
      ],
      "id": "3fe7bfb5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.16)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygQU-FEZf052"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertForSequenceClassification"
      ],
      "id": "ygQU-FEZf052",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fukxbRIQf98i",
        "outputId": "910f827e-d622-450e-f158-65ae0e227124"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "fukxbRIQf98i",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5BHR8egh78X",
        "outputId": "f381b61d-af87-42df-cc0a-9e52e43ebe0a"
      },
      "source": [
        "!unzip drive/MyDrive/MSR_data_0831/data_0831.zip"
      ],
      "id": "I5BHR8egh78X",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  drive/MyDrive/MSR_data_0831/data_0831.zip\n",
            "replace __MACOSX/._0831? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: __MACOSX/._0831         \n",
            "  inflating: 0831/.DS_Store          \n",
            "  inflating: __MACOSX/0831/._.DS_Store  \n",
            "  inflating: 0831/I_tensor20210829   \n",
            "  inflating: __MACOSX/0831/._I_tensor20210829  \n",
            "  inflating: 0831/df_csv_joined20180831.csv  \n",
            "  inflating: __MACOSX/0831/._df_csv_joined20180831.csv  \n",
            "  inflating: 0831/data_tensor_fnl20210831  \n",
            "  inflating: __MACOSX/0831/._data_tensor_fnl20210831  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHTB1Uc2CD-k",
        "outputId": "42dbc4a5-3440-4166-c2c2-728e5e0b0d6b"
      },
      "source": [
        "!unzip drive/MyDrive/MSR_data_0831/data_0831.zip > /dev/null"
      ],
      "id": "hHTB1Uc2CD-k",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "replace __MACOSX/._0831? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b530e39b"
      },
      "source": [
        "#url_df_read = pd.read_csv('drive/MyDrive/MSR_data_0831/df_csv_joined20180828.csv', index_col=[0])\n",
        "#url_df_read"
      ],
      "id": "b530e39b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81142971"
      },
      "source": [
        "#MSR_df_read = pd.read_csv('drive/MyDrive/MSR_data_0831/MSR_text_2.csv', index_col=[0])\n",
        "#MSR_df_read"
      ],
      "id": "81142971",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d418a32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ad102671-2919-4b9f-916a-308840da7118"
      },
      "source": [
        "df_csv_joined = pd.read_csv('drive/MyDrive/MSR_data_0831/df_csv_joined20180831.csv')\n",
        "df_csv_joined"
      ],
      "id": "5d418a32",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>level_0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>index</th>\n",
              "      <th>Answer ID</th>\n",
              "      <th>Answer Page</th>\n",
              "      <th>Metric</th>\n",
              "      <th>Company</th>\n",
              "      <th>Value</th>\n",
              "      <th>label</th>\n",
              "      <th>Text</th>\n",
              "      <th>Source Page</th>\n",
              "      <th>URL_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6382834</td>\n",
              "      <td>https://wikirate.org/~6382834</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>S A Brain Company Ltd</td>\n",
              "      <td>Whistleblower protection (direct employees), W...</td>\n",
              "      <td>1</td>\n",
              "      <td>10/19/2020\\n\\nSlavery and Human trafﬁcking | S...</td>\n",
              "      <td>https://wikirate.org/~6375302</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/637...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6368402</td>\n",
              "      <td>https://wikirate.org/~6368402</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Pension Protection Fund</td>\n",
              "      <td>Whistleblower protection (direct employees), I...</td>\n",
              "      <td>1</td>\n",
              "      <td>\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nModern Slav...</td>\n",
              "      <td>https://wikirate.org/~6355100</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/635...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6380781</td>\n",
              "      <td>https://wikirate.org/~6380781</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Hall &amp; Woodhouse Limited</td>\n",
              "      <td>Whistleblower protection (direct employees)</td>\n",
              "      <td>1</td>\n",
              "      <td>10/19/2020\\n\\nSlavery | Hall and Woodhouse\\n\\n...</td>\n",
              "      <td>https://wikirate.org/~6375083</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/637...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6364508</td>\n",
              "      <td>https://wikirate.org/~6364508</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Greene King</td>\n",
              "      <td>Whistleblower protection (direct employees)</td>\n",
              "      <td>1</td>\n",
              "      <td>Greene King | MODERN SLAVERY STATEMENT\\n\\nhttp...</td>\n",
              "      <td>https://wikirate.org/~6357513</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/635...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6379525</td>\n",
              "      <td>https://wikirate.org/~6379525</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Charles Wells</td>\n",
              "      <td>Whistleblower protection (direct employees)</td>\n",
              "      <td>1</td>\n",
              "      <td>Charles Wells Ltd \\n \\nModern Slavery and Huma...</td>\n",
              "      <td>https://wikirate.org/~6354153</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/635...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1637</th>\n",
              "      <td>1650</td>\n",
              "      <td>1651</td>\n",
              "      <td>1651</td>\n",
              "      <td>1713</td>\n",
              "      <td>7311793</td>\n",
              "      <td>https://wikirate.org/~7311793</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>ECO Animal Health Group Plc</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "      <td>\\n\\n \\nModern Slavery Act 2015 Statement   \\n...</td>\n",
              "      <td>https://wikirate.org/~7307585</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1638</th>\n",
              "      <td>1651</td>\n",
              "      <td>1652</td>\n",
              "      <td>1652</td>\n",
              "      <td>1714</td>\n",
              "      <td>7310878</td>\n",
              "      <td>https://wikirate.org/~7310878</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Victoria Plum Limited</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "      <td>Upto 60% off + 20% off ends\\n\\n \\n\\n \\n\\nSearc...</td>\n",
              "      <td>https://wikirate.org/~7307593</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1639</th>\n",
              "      <td>1652</td>\n",
              "      <td>1653</td>\n",
              "      <td>1653</td>\n",
              "      <td>1715</td>\n",
              "      <td>7310802</td>\n",
              "      <td>https://wikirate.org/~7310802</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Creagh Concrete Products Limited</td>\n",
              "      <td>Whistleblower protection (direct employees)</td>\n",
              "      <td>1</td>\n",
              "      <td>\\n\\nSlavery and Human Trafficking Transparenc...</td>\n",
              "      <td>https://wikirate.org/~7307608</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1640</th>\n",
              "      <td>1653</td>\n",
              "      <td>1654</td>\n",
              "      <td>1654</td>\n",
              "      <td>1716</td>\n",
              "      <td>7311831</td>\n",
              "      <td>https://wikirate.org/~7311831</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Reward Gateway (UK) Ltd</td>\n",
              "      <td>Hotline (direct employees), Whistleblower prot...</td>\n",
              "      <td>1</td>\n",
              "      <td>Reward Gateway Slavery\\nand Human Trafficking\\...</td>\n",
              "      <td>https://wikirate.org/~7307616</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1641</th>\n",
              "      <td>1654</td>\n",
              "      <td>1655</td>\n",
              "      <td>1655</td>\n",
              "      <td>1717</td>\n",
              "      <td>7309810</td>\n",
              "      <td>https://wikirate.org/~7309810</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Origin Fertilisers</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "      <td>Modern Slavery Act Statement\\n– January 2021\\n...</td>\n",
              "      <td>https://wikirate.org/~7307078</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1642 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  ...                                           URL_link\n",
              "0              0  ...  https://dq06ugkuram52.cloudfront.net/files/637...\n",
              "1              1  ...  https://dq06ugkuram52.cloudfront.net/files/635...\n",
              "2              2  ...  https://dq06ugkuram52.cloudfront.net/files/637...\n",
              "3              3  ...  https://dq06ugkuram52.cloudfront.net/files/635...\n",
              "4              4  ...  https://dq06ugkuram52.cloudfront.net/files/635...\n",
              "...          ...  ...                                                ...\n",
              "1637        1650  ...  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "1638        1651  ...  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "1639        1652  ...  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "1640        1653  ...  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "1641        1654  ...  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "\n",
              "[1642 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q68Jcd-zcKCH"
      },
      "source": [
        "short_doc = []\n",
        "for i in range(len(df_csv_joined['Text'])):\n",
        "  if len(df_csv_joined['Text'][i]) < 1500:\n",
        "    short_doc.append(i)\n"
      ],
      "id": "Q68Jcd-zcKCH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BuPWDOYAd3an",
        "outputId": "b530f648-9971-4200-ce64-3302be09ab7d"
      },
      "source": [
        "df = df_csv_joined.drop(short_doc)\n",
        "df"
      ],
      "id": "BuPWDOYAd3an",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>level_0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>index</th>\n",
              "      <th>Answer ID</th>\n",
              "      <th>Answer Page</th>\n",
              "      <th>Metric</th>\n",
              "      <th>Company</th>\n",
              "      <th>Value</th>\n",
              "      <th>label</th>\n",
              "      <th>Text</th>\n",
              "      <th>Source Page</th>\n",
              "      <th>URL_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6382834</td>\n",
              "      <td>https://wikirate.org/~6382834</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>S A Brain Company Ltd</td>\n",
              "      <td>Whistleblower protection (direct employees), W...</td>\n",
              "      <td>1</td>\n",
              "      <td>10/19/2020\\n\\nSlavery and Human trafﬁcking | S...</td>\n",
              "      <td>https://wikirate.org/~6375302</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/637...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6368402</td>\n",
              "      <td>https://wikirate.org/~6368402</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Pension Protection Fund</td>\n",
              "      <td>Whistleblower protection (direct employees), I...</td>\n",
              "      <td>1</td>\n",
              "      <td>\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nModern Slav...</td>\n",
              "      <td>https://wikirate.org/~6355100</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/635...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6380781</td>\n",
              "      <td>https://wikirate.org/~6380781</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Hall &amp; Woodhouse Limited</td>\n",
              "      <td>Whistleblower protection (direct employees)</td>\n",
              "      <td>1</td>\n",
              "      <td>10/19/2020\\n\\nSlavery | Hall and Woodhouse\\n\\n...</td>\n",
              "      <td>https://wikirate.org/~6375083</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/637...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6364508</td>\n",
              "      <td>https://wikirate.org/~6364508</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Greene King</td>\n",
              "      <td>Whistleblower protection (direct employees)</td>\n",
              "      <td>1</td>\n",
              "      <td>Greene King | MODERN SLAVERY STATEMENT\\n\\nhttp...</td>\n",
              "      <td>https://wikirate.org/~6357513</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/635...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>6379525</td>\n",
              "      <td>https://wikirate.org/~6379525</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Charles Wells</td>\n",
              "      <td>Whistleblower protection (direct employees)</td>\n",
              "      <td>1</td>\n",
              "      <td>Charles Wells Ltd \\n \\nModern Slavery and Huma...</td>\n",
              "      <td>https://wikirate.org/~6354153</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/635...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1637</th>\n",
              "      <td>1650</td>\n",
              "      <td>1651</td>\n",
              "      <td>1651</td>\n",
              "      <td>1713</td>\n",
              "      <td>7311793</td>\n",
              "      <td>https://wikirate.org/~7311793</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>ECO Animal Health Group Plc</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "      <td>\\n\\n \\nModern Slavery Act 2015 Statement   \\n...</td>\n",
              "      <td>https://wikirate.org/~7307585</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1638</th>\n",
              "      <td>1651</td>\n",
              "      <td>1652</td>\n",
              "      <td>1652</td>\n",
              "      <td>1714</td>\n",
              "      <td>7310878</td>\n",
              "      <td>https://wikirate.org/~7310878</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Victoria Plum Limited</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "      <td>Upto 60% off + 20% off ends\\n\\n \\n\\n \\n\\nSearc...</td>\n",
              "      <td>https://wikirate.org/~7307593</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1639</th>\n",
              "      <td>1652</td>\n",
              "      <td>1653</td>\n",
              "      <td>1653</td>\n",
              "      <td>1715</td>\n",
              "      <td>7310802</td>\n",
              "      <td>https://wikirate.org/~7310802</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Creagh Concrete Products Limited</td>\n",
              "      <td>Whistleblower protection (direct employees)</td>\n",
              "      <td>1</td>\n",
              "      <td>\\n\\nSlavery and Human Trafficking Transparenc...</td>\n",
              "      <td>https://wikirate.org/~7307608</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1640</th>\n",
              "      <td>1653</td>\n",
              "      <td>1654</td>\n",
              "      <td>1654</td>\n",
              "      <td>1716</td>\n",
              "      <td>7311831</td>\n",
              "      <td>https://wikirate.org/~7311831</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Reward Gateway (UK) Ltd</td>\n",
              "      <td>Hotline (direct employees), Whistleblower prot...</td>\n",
              "      <td>1</td>\n",
              "      <td>Reward Gateway Slavery\\nand Human Trafficking\\...</td>\n",
              "      <td>https://wikirate.org/~7307616</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1641</th>\n",
              "      <td>1654</td>\n",
              "      <td>1655</td>\n",
              "      <td>1655</td>\n",
              "      <td>1717</td>\n",
              "      <td>7309810</td>\n",
              "      <td>https://wikirate.org/~7309810</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Origin Fertilisers</td>\n",
              "      <td>No</td>\n",
              "      <td>0</td>\n",
              "      <td>Modern Slavery Act Statement\\n– January 2021\\n...</td>\n",
              "      <td>https://wikirate.org/~7307078</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1624 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  ...                                           URL_link\n",
              "0              0  ...  https://dq06ugkuram52.cloudfront.net/files/637...\n",
              "1              1  ...  https://dq06ugkuram52.cloudfront.net/files/635...\n",
              "2              2  ...  https://dq06ugkuram52.cloudfront.net/files/637...\n",
              "3              3  ...  https://dq06ugkuram52.cloudfront.net/files/635...\n",
              "4              4  ...  https://dq06ugkuram52.cloudfront.net/files/635...\n",
              "...          ...  ...                                                ...\n",
              "1637        1650  ...  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "1638        1651  ...  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "1639        1652  ...  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "1640        1653  ...  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "1641        1654  ...  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "\n",
              "[1624 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37ed49be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "887a509f-831b-4a63-f054-6673eae5da3d"
      },
      "source": [
        "data_tensor = torch.load('drive/MyDrive/MSR_data_0831/data_tensor_fnl20210831')\n",
        "data_tensor_np = data_tensor.detach().numpy()\n",
        "data_tensor_np1 = np.delete(data_tensor_np, short_doc, axis=0)\n",
        "\n",
        "print(data_tensor_np1.shape)\n",
        "\n",
        "data_tensor_np2 = torch.tensor(data_tensor_np1)\n",
        "\n",
        "print(data_tensor_np2.size())"
      ],
      "id": "37ed49be",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1624, 200, 768)\n",
            "torch.Size([1624, 200, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4d21773"
      },
      "source": [
        "#### I Tensor"
      ],
      "id": "b4d21773"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a9870c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d139db85-3dd1-47cf-dd40-a265546dab85"
      },
      "source": [
        "I_tensor = torch.load('drive/MyDrive/MSR_data_0831/I_tensor20210829')\n",
        "I_tensor_np = I_tensor.detach().numpy()\n",
        "I_tensor_np1 = np.delete(I_tensor_np, short_doc, axis=0)\n",
        "\n",
        "print(I_tensor_np1.shape)\n",
        "\n",
        "I_tensor1 = torch.tensor(I_tensor_np1)\n",
        "\n",
        "print(I_tensor1.size())\n"
      ],
      "id": "2a9870c1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1624, 1, 200)\n",
            "torch.Size([1624, 1, 200])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50b22995"
      },
      "source": [
        "### Model training"
      ],
      "id": "50b22995"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbf0497f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c072cf55-1628-437a-e663-032a79381770"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "id": "fbf0497f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gfTDR1LcWyB",
        "outputId": "01e8cb71-ae56-4e1c-ddac-02663575755c"
      },
      "source": [
        "label = df['label'].tolist()\n",
        "len(label)"
      ],
      "id": "1gfTDR1LcWyB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1624"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a98c0ee3"
      },
      "source": [
        "label = df['label'].tolist()\n",
        "\n",
        "train_x = data_tensor_np2[:1000]\n",
        "train_x = train_x.permute(0,2,1)\n",
        "train_y = torch.tensor(label[:1000])\n",
        "train_i = I_tensor1[:1000]\n",
        "\n",
        "val_x = data_tensor_np2[1000:]\n",
        "val_x = val_x.permute(0,2,1)\n",
        "val_y = torch.tensor(label[1000:])\n",
        "val_i = I_tensor1[1000:]\n",
        "\n",
        "test_x = data_tensor_np2[1000:]\n",
        "test_x = test_x.permute(0,2,1)\n",
        "test_y = torch.tensor(label[1000:])\n",
        "test_i = I_tensor1[1000:]"
      ],
      "id": "a98c0ee3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ea39972"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "train_data = TensorDataset(train_x, train_y, train_i)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "val_data = TensorDataset(val_x, val_y, val_i)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size, shuffle=False)"
      ],
      "id": "7ea39972",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be4f5263"
      },
      "source": [
        "class model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        \n",
        "        super(model, self).__init__()\n",
        "                        \n",
        "        self.conv1 = nn.Conv1d(in_channels = 768, out_channels=1, kernel_size=1, stride=1, padding=0)\n",
        "        \n",
        "        self.conv2 = nn.Conv1d(in_channels = 768, out_channels=512, kernel_size=1, stride=1, padding=0)\n",
        "        \n",
        "        self.fc1 = nn.Linear(512, 512)\n",
        "        \n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "                \n",
        "        self.softmax1 = nn.Softmax(dim=-1)\n",
        "        \n",
        "        self.relu =  nn.ReLU()\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, x, y, i):\n",
        "        \n",
        "        lbda = 5\n",
        "                        \n",
        "        alpha = self.conv1(x) #(btch_sz, 768, 200)-> (btch_sz, 1, 200)\n",
        "        \n",
        "        x_2 = self.conv2(x)\n",
        "        \n",
        "        x_2 = self.relu(x_2)\n",
        "                                                        \n",
        "        alpha1 = alpha + lbda * i #(btch_sz, 1, 200) + (btch_sz,1, 200)*(btch_sz, 1, 200) \n",
        "                                \n",
        "        alpha_prime = self.softmax1(alpha1) #(btch_sz, 1, 200)\n",
        "                        \n",
        "        alpha_prime_permute = alpha_prime.permute(0,2,1) #(btch_sz, 200, 1) \n",
        "                                        \n",
        "        h = torch.matmul(x_2, alpha_prime_permute)  #(btch_sz, 768, 200)*(btch_sz, 200, 1) = (btch_sz, 768, 1)\n",
        "                                \n",
        "        h = h.view(h.shape[0], -1) #(btch_sz, 768)\n",
        "                        \n",
        "        output = self.relu(self.fc1(h)) #(btch_sz, 768, 512) \n",
        "        \n",
        "        output = self.dropout(output)\n",
        "        \n",
        "        output = self.fc2(output)\n",
        "                                 \n",
        "        return output"
      ],
      "id": "be4f5263",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23296af6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92628f42-a523-43d5-a212-d5638576eac4"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        " \n",
        "class_wts = compute_class_weight('balanced', classes= torch.unique(train_y).tolist(), y=train_y.tolist())\n",
        "\n",
        "print(class_wts)\n",
        "\n",
        "weights = torch.tensor(class_wts, dtype = torch.float)\n",
        "weights = weights.to(device)\n"
      ],
      "id": "23296af6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.03519669 0.96711799]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9-CQ-RWP6kX"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "Model = model()\n",
        "\n",
        "Model = Model.to(device)\n",
        "\n",
        "cross_entropy = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "from transformers import AdamW\n",
        "\n",
        "optimizer = optim.AdamW(Model.parameters(), lr = 0.0005, weight_decay=0.01)\n",
        "\n",
        "epochs = 200\n"
      ],
      "id": "R9-CQ-RWP6kX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f0f0d18"
      },
      "source": [
        "def train():\n",
        "    Model.train()\n",
        "    \n",
        "    total_loss = 0\n",
        "\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    \n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        batch = [r.to(device) for r in batch]\n",
        " \n",
        "        x, y, i = batch\n",
        "\n",
        "        Model.zero_grad()        \n",
        "\n",
        "        preds = Model(x, y, i)\n",
        "\n",
        "        loss = cross_entropy(preds, y)\n",
        "\n",
        "        total_loss = total_loss + loss.item()\n",
        "     \n",
        "        loss.backward()\n",
        "\n",
        "        #torch.nn.utils.clip_grad_norm_(Model.parameters(), 1.0) \n",
        "\n",
        "        optimizer.step()  # update parameters\n",
        "\n",
        "        _, predicted = torch.max(preds.data, 1)\n",
        "        train_total += y.size(0)\n",
        "        train_correct += (predicted == y).sum().item()\n",
        "\n",
        "        preds=preds.detach().cpu().numpy()   # if model predictions are stored on GPU, push it to CPU\n",
        "        \n",
        "    avg_loss = total_loss / len(train_dataloader) # compute the training loss of the epoch\n",
        "    train_acc = train_correct/train_total\n",
        "  \n",
        "    return avg_loss, train_acc #returns train loss and predictions\n"
      ],
      "id": "6f0f0d18",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55179695"
      },
      "source": [
        "def evaluate():\n",
        "    \n",
        "    print(\"\\nEvaluating...\")\n",
        "    Model.eval()   # deactivate dropout layers\n",
        "    \n",
        "    total_loss = 0\n",
        "\n",
        "    val_total = 0\n",
        "    val_correct = 0\n",
        "\n",
        "    for step,batch in enumerate(val_dataloader):   # iterate over batches\n",
        "        \n",
        "        batch = [t.to(device) for t in batch] # push the batch to gpu\n",
        "\n",
        "        x, y, i = batch\n",
        "        \n",
        "        with torch.no_grad(): # deactivate autograd\n",
        "            \n",
        "            preds = Model(x, y, i)\n",
        "            \n",
        "            loss = cross_entropy(preds,y)\n",
        "\n",
        "            total_loss = total_loss + loss.item()\n",
        "\n",
        "            _, predicted = torch.max(preds.data, 1)\n",
        "            val_total += y.size(0)\n",
        "            val_correct += (predicted == y).sum().item()\n",
        "\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "            \n",
        "    avg_loss = total_loss / len(val_dataloader) \n",
        "    val_acc = val_correct/val_total\n",
        "\n",
        "    return avg_loss, val_acc"
      ],
      "id": "55179695",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f71e6bdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "457c5ff2-a3be-4e9b-f7ca-da1d13686f74"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#best_valid_loss = float('inf')  # set initial loss to infinite\n",
        "\n",
        "best_valid_acc = 0\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "train_acces = []\n",
        "val_acces = []\n",
        "\n",
        "for epoch in range(epochs): #for each epoch\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "\n",
        "    train_loss, train_acc = train() #train model\n",
        "    valid_loss, val_acc = evaluate() #evaluate model\n",
        "    \n",
        "    #if valid_loss < best_valid_loss: #save the best model\n",
        "        #best_valid_loss = valid_loss\n",
        "        #torch.save(Model.state_dict(), 'saved_weights.pt')\n",
        "\n",
        "    if val_acc > best_valid_acc: \n",
        "        best_valid_acc = val_acc\n",
        "        torch.save(Model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    \n",
        "    train_acces.append(train_acc)\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    val_acces.append(val_acces)\n",
        "    \n",
        "    print('Train acc', np.round(train_acc, 3), 'Val acc', np.round(val_acc, 3))\n",
        "    print('Train loss', np.round(train_loss, 3), 'Val loss', np.round(valid_loss, 3))\n"
      ],
      "id": "f71e6bdb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.647 Val acc 0.678\n",
            "Train loss 0.614 Val loss 0.611\n",
            "\n",
            " Epoch 2 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.731 Val acc 0.691\n",
            "Train loss 0.526 Val loss 0.638\n",
            "\n",
            " Epoch 3 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.75 Val acc 0.688\n",
            "Train loss 0.509 Val loss 0.637\n",
            "\n",
            " Epoch 4 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.761 Val acc 0.696\n",
            "Train loss 0.497 Val loss 0.63\n",
            "\n",
            " Epoch 5 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.769 Val acc 0.686\n",
            "Train loss 0.486 Val loss 0.651\n",
            "\n",
            " Epoch 6 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.781 Val acc 0.689\n",
            "Train loss 0.472 Val loss 0.681\n",
            "\n",
            " Epoch 7 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.779 Val acc 0.7\n",
            "Train loss 0.478 Val loss 0.662\n",
            "\n",
            " Epoch 8 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.791 Val acc 0.7\n",
            "Train loss 0.461 Val loss 0.704\n",
            "\n",
            " Epoch 9 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.779 Val acc 0.692\n",
            "Train loss 0.471 Val loss 0.729\n",
            "\n",
            " Epoch 10 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.774 Val acc 0.679\n",
            "Train loss 0.471 Val loss 0.717\n",
            "\n",
            " Epoch 11 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.785 Val acc 0.681\n",
            "Train loss 0.457 Val loss 0.751\n",
            "\n",
            " Epoch 12 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.786 Val acc 0.692\n",
            "Train loss 0.453 Val loss 0.708\n",
            "\n",
            " Epoch 13 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.789 Val acc 0.688\n",
            "Train loss 0.455 Val loss 0.741\n",
            "\n",
            " Epoch 14 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.796 Val acc 0.679\n",
            "Train loss 0.44 Val loss 0.834\n",
            "\n",
            " Epoch 15 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.784 Val acc 0.681\n",
            "Train loss 0.455 Val loss 0.818\n",
            "\n",
            " Epoch 16 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.802 Val acc 0.679\n",
            "Train loss 0.442 Val loss 0.815\n",
            "\n",
            " Epoch 17 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.8 Val acc 0.671\n",
            "Train loss 0.441 Val loss 0.976\n",
            "\n",
            " Epoch 18 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.788 Val acc 0.678\n",
            "Train loss 0.448 Val loss 0.912\n",
            "\n",
            " Epoch 19 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.763 Val acc 0.673\n",
            "Train loss 0.469 Val loss 0.902\n",
            "\n",
            " Epoch 20 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.777 Val acc 0.676\n",
            "Train loss 0.452 Val loss 0.863\n",
            "\n",
            " Epoch 21 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.793 Val acc 0.676\n",
            "Train loss 0.44 Val loss 0.894\n",
            "\n",
            " Epoch 22 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.793 Val acc 0.679\n",
            "Train loss 0.426 Val loss 0.904\n",
            "\n",
            " Epoch 23 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.789 Val acc 0.67\n",
            "Train loss 0.433 Val loss 0.908\n",
            "\n",
            " Epoch 24 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.795 Val acc 0.689\n",
            "Train loss 0.438 Val loss 0.885\n",
            "\n",
            " Epoch 25 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.805 Val acc 0.684\n",
            "Train loss 0.419 Val loss 0.942\n",
            "\n",
            " Epoch 26 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.814 Val acc 0.697\n",
            "Train loss 0.416 Val loss 0.946\n",
            "\n",
            " Epoch 27 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.782 Val acc 0.692\n",
            "Train loss 0.45 Val loss 0.915\n",
            "\n",
            " Epoch 28 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.79 Val acc 0.614\n",
            "Train loss 0.448 Val loss 0.997\n",
            "\n",
            " Epoch 29 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.787 Val acc 0.694\n",
            "Train loss 0.433 Val loss 0.974\n",
            "\n",
            " Epoch 30 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.809 Val acc 0.647\n",
            "Train loss 0.418 Val loss 0.997\n",
            "\n",
            " Epoch 31 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.797 Val acc 0.673\n",
            "Train loss 0.43 Val loss 1.002\n",
            "\n",
            " Epoch 32 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.81 Val acc 0.689\n",
            "Train loss 0.418 Val loss 0.986\n",
            "\n",
            " Epoch 33 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.8 Val acc 0.686\n",
            "Train loss 0.433 Val loss 0.941\n",
            "\n",
            " Epoch 34 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.805 Val acc 0.678\n",
            "Train loss 0.423 Val loss 0.996\n",
            "\n",
            " Epoch 35 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.811 Val acc 0.689\n",
            "Train loss 0.413 Val loss 0.978\n",
            "\n",
            " Epoch 36 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.801 Val acc 0.67\n",
            "Train loss 0.426 Val loss 1.073\n",
            "\n",
            " Epoch 37 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.804 Val acc 0.696\n",
            "Train loss 0.429 Val loss 1.027\n",
            "\n",
            " Epoch 38 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.782 Val acc 0.691\n",
            "Train loss 0.47 Val loss 0.936\n",
            "\n",
            " Epoch 39 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.801 Val acc 0.668\n",
            "Train loss 0.434 Val loss 0.939\n",
            "\n",
            " Epoch 40 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.785 Val acc 0.657\n",
            "Train loss 0.445 Val loss 0.939\n",
            "\n",
            " Epoch 41 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.805 Val acc 0.681\n",
            "Train loss 0.428 Val loss 0.905\n",
            "\n",
            " Epoch 42 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.814 Val acc 0.699\n",
            "Train loss 0.41 Val loss 0.939\n",
            "\n",
            " Epoch 43 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.815 Val acc 0.689\n",
            "Train loss 0.403 Val loss 1.035\n",
            "\n",
            " Epoch 44 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.81 Val acc 0.679\n",
            "Train loss 0.418 Val loss 1.03\n",
            "\n",
            " Epoch 45 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.803 Val acc 0.691\n",
            "Train loss 0.416 Val loss 1.017\n",
            "\n",
            " Epoch 46 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.821 Val acc 0.696\n",
            "Train loss 0.4 Val loss 0.996\n",
            "\n",
            " Epoch 47 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.801 Val acc 0.688\n",
            "Train loss 0.414 Val loss 0.974\n",
            "\n",
            " Epoch 48 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.821 Val acc 0.697\n",
            "Train loss 0.393 Val loss 1.042\n",
            "\n",
            " Epoch 49 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.823 Val acc 0.7\n",
            "Train loss 0.395 Val loss 1.008\n",
            "\n",
            " Epoch 50 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.817 Val acc 0.654\n",
            "Train loss 0.405 Val loss 1.023\n",
            "\n",
            " Epoch 51 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.821 Val acc 0.702\n",
            "Train loss 0.395 Val loss 1.062\n",
            "\n",
            " Epoch 52 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.812 Val acc 0.631\n",
            "Train loss 0.401 Val loss 1.092\n",
            "\n",
            " Epoch 53 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.793 Val acc 0.686\n",
            "Train loss 0.432 Val loss 1.007\n",
            "\n",
            " Epoch 54 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.824 Val acc 0.678\n",
            "Train loss 0.393 Val loss 1.025\n",
            "\n",
            " Epoch 55 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.795 Val acc 0.702\n",
            "Train loss 0.419 Val loss 1.02\n",
            "\n",
            " Epoch 56 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.818 Val acc 0.691\n",
            "Train loss 0.402 Val loss 1.053\n",
            "\n",
            " Epoch 57 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.815 Val acc 0.678\n",
            "Train loss 0.386 Val loss 1.182\n",
            "\n",
            " Epoch 58 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.808 Val acc 0.684\n",
            "Train loss 0.413 Val loss 1.109\n",
            "\n",
            " Epoch 59 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.792 Val acc 0.681\n",
            "Train loss 0.422 Val loss 1.211\n",
            "\n",
            " Epoch 60 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.819 Val acc 0.688\n",
            "Train loss 0.399 Val loss 1.167\n",
            "\n",
            " Epoch 61 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.823 Val acc 0.683\n",
            "Train loss 0.396 Val loss 1.118\n",
            "\n",
            " Epoch 62 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.829 Val acc 0.691\n",
            "Train loss 0.381 Val loss 1.144\n",
            "\n",
            " Epoch 63 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.828 Val acc 0.675\n",
            "Train loss 0.383 Val loss 1.137\n",
            "\n",
            " Epoch 64 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.828 Val acc 0.688\n",
            "Train loss 0.376 Val loss 1.131\n",
            "\n",
            " Epoch 65 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.821 Val acc 0.696\n",
            "Train loss 0.384 Val loss 1.189\n",
            "\n",
            " Epoch 66 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.827 Val acc 0.67\n",
            "Train loss 0.38 Val loss 1.162\n",
            "\n",
            " Epoch 67 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.83 Val acc 0.681\n",
            "Train loss 0.379 Val loss 1.123\n",
            "\n",
            " Epoch 68 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.833 Val acc 0.694\n",
            "Train loss 0.378 Val loss 1.193\n",
            "\n",
            " Epoch 69 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.831 Val acc 0.654\n",
            "Train loss 0.379 Val loss 1.122\n",
            "\n",
            " Epoch 70 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.831 Val acc 0.652\n",
            "Train loss 0.378 Val loss 1.171\n",
            "\n",
            " Epoch 71 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.827 Val acc 0.696\n",
            "Train loss 0.387 Val loss 1.239\n",
            "\n",
            " Epoch 72 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.818 Val acc 0.615\n",
            "Train loss 0.39 Val loss 1.222\n",
            "\n",
            " Epoch 73 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.812 Val acc 0.675\n",
            "Train loss 0.392 Val loss 1.159\n",
            "\n",
            " Epoch 74 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.805 Val acc 0.681\n",
            "Train loss 0.423 Val loss 1.151\n",
            "\n",
            " Epoch 75 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.767 Val acc 0.691\n",
            "Train loss 0.494 Val loss 1.113\n",
            "\n",
            " Epoch 76 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.798 Val acc 0.692\n",
            "Train loss 0.43 Val loss 1.162\n",
            "\n",
            " Epoch 77 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.821 Val acc 0.694\n",
            "Train loss 0.394 Val loss 1.182\n",
            "\n",
            " Epoch 78 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.827 Val acc 0.671\n",
            "Train loss 0.389 Val loss 1.166\n",
            "\n",
            " Epoch 79 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.805 Val acc 0.67\n",
            "Train loss 0.403 Val loss 1.196\n",
            "\n",
            " Epoch 80 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.822 Val acc 0.683\n",
            "Train loss 0.385 Val loss 1.251\n",
            "\n",
            " Epoch 81 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.843 Val acc 0.676\n",
            "Train loss 0.373 Val loss 1.207\n",
            "\n",
            " Epoch 82 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.825 Val acc 0.689\n",
            "Train loss 0.38 Val loss 1.253\n",
            "\n",
            " Epoch 83 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.831 Val acc 0.683\n",
            "Train loss 0.372 Val loss 1.182\n",
            "\n",
            " Epoch 84 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.841 Val acc 0.675\n",
            "Train loss 0.369 Val loss 1.346\n",
            "\n",
            " Epoch 85 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.837 Val acc 0.691\n",
            "Train loss 0.364 Val loss 1.218\n",
            "\n",
            " Epoch 86 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.835 Val acc 0.697\n",
            "Train loss 0.364 Val loss 1.288\n",
            "\n",
            " Epoch 87 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.816 Val acc 0.694\n",
            "Train loss 0.403 Val loss 1.272\n",
            "\n",
            " Epoch 88 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.808 Val acc 0.651\n",
            "Train loss 0.417 Val loss 1.245\n",
            "\n",
            " Epoch 89 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.816 Val acc 0.686\n",
            "Train loss 0.383 Val loss 1.312\n",
            "\n",
            " Epoch 90 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.839 Val acc 0.691\n",
            "Train loss 0.373 Val loss 1.257\n",
            "\n",
            " Epoch 91 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.84 Val acc 0.683\n",
            "Train loss 0.363 Val loss 1.257\n",
            "\n",
            " Epoch 92 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.829 Val acc 0.679\n",
            "Train loss 0.367 Val loss 1.443\n",
            "\n",
            " Epoch 93 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.836 Val acc 0.696\n",
            "Train loss 0.375 Val loss 1.264\n",
            "\n",
            " Epoch 94 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.839 Val acc 0.675\n",
            "Train loss 0.366 Val loss 1.24\n",
            "\n",
            " Epoch 95 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.824 Val acc 0.694\n",
            "Train loss 0.391 Val loss 1.23\n",
            "\n",
            " Epoch 96 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.825 Val acc 0.646\n",
            "Train loss 0.368 Val loss 1.253\n",
            "\n",
            " Epoch 97 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.828 Val acc 0.694\n",
            "Train loss 0.362 Val loss 1.25\n",
            "\n",
            " Epoch 98 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.846 Val acc 0.678\n",
            "Train loss 0.354 Val loss 1.247\n",
            "\n",
            " Epoch 99 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.839 Val acc 0.688\n",
            "Train loss 0.358 Val loss 1.303\n",
            "\n",
            " Epoch 100 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.834 Val acc 0.691\n",
            "Train loss 0.366 Val loss 1.304\n",
            "\n",
            " Epoch 101 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.839 Val acc 0.688\n",
            "Train loss 0.358 Val loss 1.259\n",
            "\n",
            " Epoch 102 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.841 Val acc 0.692\n",
            "Train loss 0.354 Val loss 1.282\n",
            "\n",
            " Epoch 103 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.839 Val acc 0.686\n",
            "Train loss 0.353 Val loss 1.404\n",
            "\n",
            " Epoch 104 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.835 Val acc 0.702\n",
            "Train loss 0.359 Val loss 1.327\n",
            "\n",
            " Epoch 105 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.841 Val acc 0.699\n",
            "Train loss 0.353 Val loss 1.242\n",
            "\n",
            " Epoch 106 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.832 Val acc 0.643\n",
            "Train loss 0.352 Val loss 1.359\n",
            "\n",
            " Epoch 107 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.833 Val acc 0.694\n",
            "Train loss 0.361 Val loss 1.264\n",
            "\n",
            " Epoch 108 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.847 Val acc 0.684\n",
            "Train loss 0.346 Val loss 1.29\n",
            "\n",
            " Epoch 109 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.844 Val acc 0.668\n",
            "Train loss 0.357 Val loss 1.315\n",
            "\n",
            " Epoch 110 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.821 Val acc 0.67\n",
            "Train loss 0.379 Val loss 1.323\n",
            "\n",
            " Epoch 111 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.834 Val acc 0.673\n",
            "Train loss 0.371 Val loss 1.305\n",
            "\n",
            " Epoch 112 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.834 Val acc 0.667\n",
            "Train loss 0.365 Val loss 1.336\n",
            "\n",
            " Epoch 113 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.828 Val acc 0.683\n",
            "Train loss 0.359 Val loss 1.353\n",
            "\n",
            " Epoch 114 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.849 Val acc 0.689\n",
            "Train loss 0.349 Val loss 1.305\n",
            "\n",
            " Epoch 115 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.843 Val acc 0.686\n",
            "Train loss 0.353 Val loss 1.323\n",
            "\n",
            " Epoch 116 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.849 Val acc 0.688\n",
            "Train loss 0.345 Val loss 1.354\n",
            "\n",
            " Epoch 117 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.84 Val acc 0.697\n",
            "Train loss 0.366 Val loss 1.325\n",
            "\n",
            " Epoch 118 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.811 Val acc 0.63\n",
            "Train loss 0.39 Val loss 1.578\n",
            "\n",
            " Epoch 119 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.808 Val acc 0.691\n",
            "Train loss 0.405 Val loss 1.198\n",
            "\n",
            " Epoch 120 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.814 Val acc 0.704\n",
            "Train loss 0.382 Val loss 1.201\n",
            "\n",
            " Epoch 121 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.83 Val acc 0.679\n",
            "Train loss 0.381 Val loss 1.164\n",
            "\n",
            " Epoch 122 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.84 Val acc 0.691\n",
            "Train loss 0.355 Val loss 1.225\n",
            "\n",
            " Epoch 123 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.833 Val acc 0.684\n",
            "Train loss 0.368 Val loss 1.233\n",
            "\n",
            " Epoch 124 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.846 Val acc 0.692\n",
            "Train loss 0.359 Val loss 1.297\n",
            "\n",
            " Epoch 125 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.814 Val acc 0.678\n",
            "Train loss 0.385 Val loss 1.3\n",
            "\n",
            " Epoch 126 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.798 Val acc 0.679\n",
            "Train loss 0.414 Val loss 1.297\n",
            "\n",
            " Epoch 127 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.824 Val acc 0.67\n",
            "Train loss 0.376 Val loss 1.327\n",
            "\n",
            " Epoch 128 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.843 Val acc 0.679\n",
            "Train loss 0.353 Val loss 1.302\n",
            "\n",
            " Epoch 129 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.849 Val acc 0.684\n",
            "Train loss 0.344 Val loss 1.277\n",
            "\n",
            " Epoch 130 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.835 Val acc 0.679\n",
            "Train loss 0.354 Val loss 1.313\n",
            "\n",
            " Epoch 131 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.85 Val acc 0.652\n",
            "Train loss 0.34 Val loss 1.344\n",
            "\n",
            " Epoch 132 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.83 Val acc 0.617\n",
            "Train loss 0.366 Val loss 1.382\n",
            "\n",
            " Epoch 133 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.829 Val acc 0.676\n",
            "Train loss 0.365 Val loss 1.307\n",
            "\n",
            " Epoch 134 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.855 Val acc 0.679\n",
            "Train loss 0.332 Val loss 1.376\n",
            "\n",
            " Epoch 135 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.833 Val acc 0.665\n",
            "Train loss 0.362 Val loss 1.376\n",
            "\n",
            " Epoch 136 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.824 Val acc 0.67\n",
            "Train loss 0.366 Val loss 1.329\n",
            "\n",
            " Epoch 137 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.851 Val acc 0.688\n",
            "Train loss 0.337 Val loss 1.296\n",
            "\n",
            " Epoch 138 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.858 Val acc 0.691\n",
            "Train loss 0.339 Val loss 1.34\n",
            "\n",
            " Epoch 139 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.854 Val acc 0.668\n",
            "Train loss 0.323 Val loss 1.351\n",
            "\n",
            " Epoch 140 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.847 Val acc 0.668\n",
            "Train loss 0.351 Val loss 1.344\n",
            "\n",
            " Epoch 141 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.822 Val acc 0.684\n",
            "Train loss 0.362 Val loss 1.334\n",
            "\n",
            " Epoch 142 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.841 Val acc 0.675\n",
            "Train loss 0.351 Val loss 1.337\n",
            "\n",
            " Epoch 143 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.842 Val acc 0.667\n",
            "Train loss 0.355 Val loss 1.299\n",
            "\n",
            " Epoch 144 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.847 Val acc 0.671\n",
            "Train loss 0.351 Val loss 1.346\n",
            "\n",
            " Epoch 145 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.854 Val acc 0.665\n",
            "Train loss 0.336 Val loss 1.383\n",
            "\n",
            " Epoch 146 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.845 Val acc 0.676\n",
            "Train loss 0.344 Val loss 1.311\n",
            "\n",
            " Epoch 147 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.835 Val acc 0.676\n",
            "Train loss 0.364 Val loss 1.355\n",
            "\n",
            " Epoch 148 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.85 Val acc 0.684\n",
            "Train loss 0.331 Val loss 1.42\n",
            "\n",
            " Epoch 149 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.841 Val acc 0.678\n",
            "Train loss 0.347 Val loss 1.389\n",
            "\n",
            " Epoch 150 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.838 Val acc 0.655\n",
            "Train loss 0.346 Val loss 1.335\n",
            "\n",
            " Epoch 151 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.855 Val acc 0.662\n",
            "Train loss 0.331 Val loss 1.387\n",
            "\n",
            " Epoch 152 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.851 Val acc 0.688\n",
            "Train loss 0.334 Val loss 1.372\n",
            "\n",
            " Epoch 153 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.852 Val acc 0.673\n",
            "Train loss 0.319 Val loss 1.404\n",
            "\n",
            " Epoch 154 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.871 Val acc 0.689\n",
            "Train loss 0.318 Val loss 1.425\n",
            "\n",
            " Epoch 155 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.851 Val acc 0.681\n",
            "Train loss 0.321 Val loss 1.441\n",
            "\n",
            " Epoch 156 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.842 Val acc 0.683\n",
            "Train loss 0.347 Val loss 1.487\n",
            "\n",
            " Epoch 157 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.835 Val acc 0.673\n",
            "Train loss 0.349 Val loss 1.409\n",
            "\n",
            " Epoch 158 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.829 Val acc 0.657\n",
            "Train loss 0.357 Val loss 1.596\n",
            "\n",
            " Epoch 159 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.834 Val acc 0.679\n",
            "Train loss 0.353 Val loss 1.374\n",
            "\n",
            " Epoch 160 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.851 Val acc 0.676\n",
            "Train loss 0.327 Val loss 1.421\n",
            "\n",
            " Epoch 161 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.86 Val acc 0.683\n",
            "Train loss 0.322 Val loss 1.444\n",
            "\n",
            " Epoch 162 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.843 Val acc 0.689\n",
            "Train loss 0.332 Val loss 1.431\n",
            "\n",
            " Epoch 163 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.841 Val acc 0.679\n",
            "Train loss 0.346 Val loss 1.426\n",
            "\n",
            " Epoch 164 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.852 Val acc 0.689\n",
            "Train loss 0.34 Val loss 1.427\n",
            "\n",
            " Epoch 165 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.862 Val acc 0.684\n",
            "Train loss 0.323 Val loss 1.373\n",
            "\n",
            " Epoch 166 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.865 Val acc 0.678\n",
            "Train loss 0.314 Val loss 1.475\n",
            "\n",
            " Epoch 167 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.828 Val acc 0.671\n",
            "Train loss 0.361 Val loss 1.645\n",
            "\n",
            " Epoch 168 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.847 Val acc 0.683\n",
            "Train loss 0.335 Val loss 1.523\n",
            "\n",
            " Epoch 169 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.832 Val acc 0.683\n",
            "Train loss 0.348 Val loss 1.48\n",
            "\n",
            " Epoch 170 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.848 Val acc 0.688\n",
            "Train loss 0.331 Val loss 1.557\n",
            "\n",
            " Epoch 171 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.857 Val acc 0.686\n",
            "Train loss 0.32 Val loss 1.509\n",
            "\n",
            " Epoch 172 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.837 Val acc 0.692\n",
            "Train loss 0.358 Val loss 1.491\n",
            "\n",
            " Epoch 173 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.834 Val acc 0.665\n",
            "Train loss 0.354 Val loss 1.407\n",
            "\n",
            " Epoch 174 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.834 Val acc 0.7\n",
            "Train loss 0.396 Val loss 1.214\n",
            "\n",
            " Epoch 175 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.845 Val acc 0.667\n",
            "Train loss 0.395 Val loss 1.053\n",
            "\n",
            " Epoch 176 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.834 Val acc 0.692\n",
            "Train loss 0.359 Val loss 1.077\n",
            "\n",
            " Epoch 177 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.869 Val acc 0.692\n",
            "Train loss 0.318 Val loss 1.09\n",
            "\n",
            " Epoch 178 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.865 Val acc 0.692\n",
            "Train loss 0.318 Val loss 1.096\n",
            "\n",
            " Epoch 179 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.863 Val acc 0.681\n",
            "Train loss 0.315 Val loss 1.135\n",
            "\n",
            " Epoch 180 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.841 Val acc 0.696\n",
            "Train loss 0.336 Val loss 1.114\n",
            "\n",
            " Epoch 181 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.85 Val acc 0.689\n",
            "Train loss 0.328 Val loss 1.162\n",
            "\n",
            " Epoch 182 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.856 Val acc 0.696\n",
            "Train loss 0.319 Val loss 1.17\n",
            "\n",
            " Epoch 183 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.867 Val acc 0.712\n",
            "Train loss 0.306 Val loss 1.216\n",
            "\n",
            " Epoch 184 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.874 Val acc 0.691\n",
            "Train loss 0.307 Val loss 1.206\n",
            "\n",
            " Epoch 185 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.852 Val acc 0.699\n",
            "Train loss 0.315 Val loss 1.195\n",
            "\n",
            " Epoch 186 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.855 Val acc 0.696\n",
            "Train loss 0.331 Val loss 1.264\n",
            "\n",
            " Epoch 187 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.86 Val acc 0.676\n",
            "Train loss 0.314 Val loss 1.233\n",
            "\n",
            " Epoch 188 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.873 Val acc 0.655\n",
            "Train loss 0.31 Val loss 1.243\n",
            "\n",
            " Epoch 189 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.858 Val acc 0.683\n",
            "Train loss 0.321 Val loss 1.273\n",
            "\n",
            " Epoch 190 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.859 Val acc 0.691\n",
            "Train loss 0.319 Val loss 1.286\n",
            "\n",
            " Epoch 191 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.87 Val acc 0.667\n",
            "Train loss 0.3 Val loss 1.19\n",
            "\n",
            " Epoch 192 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.85 Val acc 0.694\n",
            "Train loss 0.32 Val loss 1.184\n",
            "\n",
            " Epoch 193 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.862 Val acc 0.668\n",
            "Train loss 0.32 Val loss 1.232\n",
            "\n",
            " Epoch 194 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.836 Val acc 0.641\n",
            "Train loss 0.396 Val loss 1.323\n",
            "\n",
            " Epoch 195 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.78 Val acc 0.538\n",
            "Train loss 0.576 Val loss 1.087\n",
            "\n",
            " Epoch 196 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.743 Val acc 0.662\n",
            "Train loss 0.537 Val loss 0.734\n",
            "\n",
            " Epoch 197 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.8 Val acc 0.697\n",
            "Train loss 0.429 Val loss 0.745\n",
            "\n",
            " Epoch 198 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.834 Val acc 0.692\n",
            "Train loss 0.376 Val loss 0.843\n",
            "\n",
            " Epoch 199 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.855 Val acc 0.718\n",
            "Train loss 0.34 Val loss 0.936\n",
            "\n",
            " Epoch 200 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.845 Val acc 0.712\n",
            "Train loss 0.339 Val loss 0.954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNS0HjWAKMc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "130c19d4-6142-468a-b9d2-ce201b02510e"
      },
      "source": [
        "path = 'saved_weights.pt'\n",
        "Model.load_state_dict(torch.load(path))\n",
        "\n",
        "print('Train_report:')\n",
        "with torch.no_grad():\n",
        "  train_preds = Model(train_x.to(device), train_y.to(device), train_i.to(device))\n",
        "  train_preds = train_preds.detach().cpu().numpy()\n",
        "\n",
        "train_preds = np.argmax(train_preds, axis = 1)\n",
        "print(classification_report(train_y, train_preds))\n",
        "\n",
        "\n",
        "print('Val_report:')\n",
        "with torch.no_grad():\n",
        "  val_preds = Model(val_x.to(device), val_y.to(device), val_i.to(device))\n",
        "  val_preds = val_preds.detach().cpu().numpy()\n",
        "\n",
        "val_preds = np.argmax(val_preds, axis = 1)\n",
        "print(classification_report(val_y, val_preds))\n",
        "\n",
        "print('Test_report:')\n",
        "with torch.no_grad():\n",
        "  test_preds = Model(test_x.to(device), test_y.to(device), test_i.to(device))\n",
        "  test_preds = test_preds.detach().cpu().numpy()\n",
        "\n",
        "test_preds = np.argmax(test_preds, axis = 1)\n",
        "print(classification_report(test_y, test_preds))"
      ],
      "id": "rNS0HjWAKMc1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.94      0.86       483\n",
            "           1       0.93      0.76      0.84       517\n",
            "\n",
            "    accuracy                           0.85      1000\n",
            "   macro avg       0.86      0.85      0.85      1000\n",
            "weighted avg       0.86      0.85      0.85      1000\n",
            "\n",
            "Val_report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.78      0.69       247\n",
            "           1       0.82      0.68      0.74       377\n",
            "\n",
            "    accuracy                           0.72       624\n",
            "   macro avg       0.72      0.73      0.71       624\n",
            "weighted avg       0.74      0.72      0.72       624\n",
            "\n",
            "Test_report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.78      0.69       247\n",
            "           1       0.82      0.68      0.74       377\n",
            "\n",
            "    accuracy                           0.72       624\n",
            "   macro avg       0.72      0.73      0.71       624\n",
            "weighted avg       0.74      0.72      0.72       624\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIoUTrlvBXIr"
      },
      "source": [
        "x = list(range(0,200))\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(x, train_acces)\n",
        "plt.plot(x, val_acces)\n",
        "plt.plot(x, train_losses)\n",
        "plt.plot(x, valid_losses)\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model train and validation loss')"
      ],
      "id": "JIoUTrlvBXIr",
      "execution_count": null,
      "outputs": []
    }
  ]
}