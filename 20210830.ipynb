{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "20210830.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fe7bfb5",
        "outputId": "eb71238b-9266-464b-dd59-0dca6a97c1a6"
      },
      "source": [
        "pip install transformers"
      ],
      "id": "3fe7bfb5",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygQU-FEZf052"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertForSequenceClassification"
      ],
      "id": "ygQU-FEZf052",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fukxbRIQf98i",
        "outputId": "c299c62e-02c4-43a8-86f8-09ac696018ac"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "fukxbRIQf98i",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5BHR8egh78X",
        "outputId": "0e7d989d-7721-472d-d932-779a343c138f"
      },
      "source": [
        "!unzip drive/MyDrive/MSR_data/MSR_Project_files.zip"
      ],
      "id": "I5BHR8egh78X",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/MyDrive/MSR_data/MSR_Project_files.zip\n",
            "replace __MACOSX/._MSR Project files ? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: __MACOSX/._MSR Project files   \n",
            "  inflating: MSR Project files /VerifiedDataLabels.csv  \n",
            "  inflating: __MACOSX/MSR Project files /._VerifiedDataLabels.csv  \n",
            "  inflating: MSR Project files /url_df.csv  \n",
            "  inflating: __MACOSX/MSR Project files /._url_df.csv  \n",
            "  inflating: MSR Project files /.DS_Store  \n",
            "  inflating: __MACOSX/MSR Project files /._.DS_Store  \n",
            "  inflating: MSR Project files /data_tensor_fnl  A\n",
            "\n",
            "  inflating: __MACOSX/MSR Project files /._data_tensor_fnl  \n",
            "  inflating: MSR Project files /stem_sentence_corpus  \n",
            "  inflating: __MACOSX/MSR Project files /._stem_sentence_corpus  \n",
            "  inflating: MSR Project files /I_tensor  \n",
            "  inflating: __MACOSX/MSR Project files /._I_tensor  \n",
            "  inflating: MSR Project files /df_csv_joined.csv  \n",
            "  inflating: __MACOSX/MSR Project files /._df_csv_joined.csv  \n",
            "  inflating: MSR Project files /MSR_text_2.csv  \n",
            "  inflating: __MACOSX/MSR Project files /._MSR_text_2.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHTB1Uc2CD-k",
        "outputId": "46cee4c6-965b-4a77-a591-bbe402c13e45"
      },
      "source": [
        "!unzip drive/MyDrive/MSR_data/MSR_Project_files.zip > /dev/null"
      ],
      "id": "hHTB1Uc2CD-k",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace __MACOSX/._MSR Project files ? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "b530e39b",
        "outputId": "6e7162f2-730f-4098-9193-3edd0653976f"
      },
      "source": [
        "url_df_read = pd.read_csv('drive/MyDrive/MSR_data/url_df.csv', index_col=[0])\n",
        "url_df_read"
      ],
      "id": "b530e39b",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>URL_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/637...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/635...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/637...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/635...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/635...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1867</th>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1868</th>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1869</th>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1870</th>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1871</th>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1872 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               URL_link\n",
              "0     https://dq06ugkuram52.cloudfront.net/files/637...\n",
              "1     https://dq06ugkuram52.cloudfront.net/files/635...\n",
              "2     https://dq06ugkuram52.cloudfront.net/files/637...\n",
              "3     https://dq06ugkuram52.cloudfront.net/files/635...\n",
              "4     https://dq06ugkuram52.cloudfront.net/files/635...\n",
              "...                                                 ...\n",
              "1867  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "1868  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "1869  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "1870  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "1871  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "\n",
              "[1872 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81142971",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "d6986bbc-3893-4756-c4af-d0f52b9b8e98"
      },
      "source": [
        "MSR_df_read = pd.read_csv('drive/MyDrive/MSR_data/MSR_text_2.csv', index_col=[0])\n",
        "MSR_df_read"
      ],
      "id": "81142971",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10/19/2020\\n\\nSlavery and Human trafﬁcking | S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nModern Slav...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10/19/2020\\n\\nSlavery | Hall and Woodhouse\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Greene King | MODERN SLAVERY STATEMENT\\n\\nhttp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Charles Wells Ltd \\n \\nModern Slavery and Huma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1867</th>\n",
              "      <td>\\n\\n \\nModern Slavery Act 2015 Statement   \\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1868</th>\n",
              "      <td>Upto 60% off + 20% off ends\\n\\n \\n\\n \\n\\nSearc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1869</th>\n",
              "      <td>\\n\\nSlavery and Human Trafficking Transparenc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1870</th>\n",
              "      <td>Reward Gateway Slavery\\nand Human Trafficking\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1871</th>\n",
              "      <td>Modern Slavery Act Statement\\n– January 2021\\n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1872 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Text\n",
              "0     10/19/2020\\n\\nSlavery and Human trafﬁcking | S...\n",
              "1      \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nModern Slav...\n",
              "2     10/19/2020\\n\\nSlavery | Hall and Woodhouse\\n\\n...\n",
              "3     Greene King | MODERN SLAVERY STATEMENT\\n\\nhttp...\n",
              "4     Charles Wells Ltd \\n \\nModern Slavery and Huma...\n",
              "...                                                 ...\n",
              "1867   \\n\\n \\nModern Slavery Act 2015 Statement   \\n...\n",
              "1868  Upto 60% off + 20% off ends\\n\\n \\n\\n \\n\\nSearc...\n",
              "1869   \\n\\nSlavery and Human Trafficking Transparenc...\n",
              "1870  Reward Gateway Slavery\\nand Human Trafficking\\...\n",
              "1871  Modern Slavery Act Statement\\n– January 2021\\n...\n",
              "\n",
              "[1872 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d418a32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 966
        },
        "outputId": "d81780da-ab3f-48e3-baa3-e3f7b76005e9"
      },
      "source": [
        "df_csv_joined = pd.read_csv('drive/MyDrive/MSR_data/df_csv_joined.csv')\n",
        "df_csv_joined"
      ],
      "id": "5d418a32",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Answer ID</th>\n",
              "      <th>Answer Page</th>\n",
              "      <th>Metric</th>\n",
              "      <th>Company</th>\n",
              "      <th>Value</th>\n",
              "      <th>Source Page</th>\n",
              "      <th>Text</th>\n",
              "      <th>URL_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6382834</td>\n",
              "      <td>https://wikirate.org/~6382834</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>S A Brain Company Ltd</td>\n",
              "      <td>1</td>\n",
              "      <td>https://wikirate.org/~6375302</td>\n",
              "      <td>10/19/2020\\n\\nSlavery and Human trafﬁcking | S...</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/637...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6368402</td>\n",
              "      <td>https://wikirate.org/~6368402</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Pension Protection Fund</td>\n",
              "      <td>1</td>\n",
              "      <td>https://wikirate.org/~6355100</td>\n",
              "      <td>\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nModern Slav...</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/635...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6380781</td>\n",
              "      <td>https://wikirate.org/~6380781</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Hall &amp; Woodhouse Limited</td>\n",
              "      <td>1</td>\n",
              "      <td>https://wikirate.org/~6375083</td>\n",
              "      <td>10/19/2020\\n\\nSlavery | Hall and Woodhouse\\n\\n...</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/637...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6364508</td>\n",
              "      <td>https://wikirate.org/~6364508</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Greene King</td>\n",
              "      <td>1</td>\n",
              "      <td>https://wikirate.org/~6357513</td>\n",
              "      <td>Greene King | MODERN SLAVERY STATEMENT\\n\\nhttp...</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/635...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6379525</td>\n",
              "      <td>https://wikirate.org/~6379525</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Charles Wells</td>\n",
              "      <td>1</td>\n",
              "      <td>https://wikirate.org/~6354153</td>\n",
              "      <td>Charles Wells Ltd \\n \\nModern Slavery and Huma...</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/635...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1713</th>\n",
              "      <td>7311793</td>\n",
              "      <td>https://wikirate.org/~7311793</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>ECO Animal Health Group Plc</td>\n",
              "      <td>0</td>\n",
              "      <td>https://wikirate.org/~7307585</td>\n",
              "      <td>\\n\\n \\nModern Slavery Act 2015 Statement   \\n...</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1714</th>\n",
              "      <td>7310878</td>\n",
              "      <td>https://wikirate.org/~7310878</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Victoria Plum Limited</td>\n",
              "      <td>0</td>\n",
              "      <td>https://wikirate.org/~7307593</td>\n",
              "      <td>Upto 60% off + 20% off ends\\n\\n \\n\\n \\n\\nSearc...</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1715</th>\n",
              "      <td>7310802</td>\n",
              "      <td>https://wikirate.org/~7310802</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Creagh Concrete Products Limited</td>\n",
              "      <td>1</td>\n",
              "      <td>https://wikirate.org/~7307608</td>\n",
              "      <td>\\n\\nSlavery and Human Trafficking Transparenc...</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1716</th>\n",
              "      <td>7311831</td>\n",
              "      <td>https://wikirate.org/~7311831</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Reward Gateway (UK) Ltd</td>\n",
              "      <td>1</td>\n",
              "      <td>https://wikirate.org/~7307616</td>\n",
              "      <td>Reward Gateway Slavery\\nand Human Trafficking\\...</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1717</th>\n",
              "      <td>7309810</td>\n",
              "      <td>https://wikirate.org/~7309810</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Origin Fertilisers</td>\n",
              "      <td>0</td>\n",
              "      <td>https://wikirate.org/~7307078</td>\n",
              "      <td>Modern Slavery Act Statement\\n– January 2021\\n...</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1718 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Answer ID  ...                                           URL_link\n",
              "0       6382834  ...  https://dq06ugkuram52.cloudfront.net/files/637...\n",
              "1       6368402  ...  https://dq06ugkuram52.cloudfront.net/files/635...\n",
              "2       6380781  ...  https://dq06ugkuram52.cloudfront.net/files/637...\n",
              "3       6364508  ...  https://dq06ugkuram52.cloudfront.net/files/635...\n",
              "4       6379525  ...  https://dq06ugkuram52.cloudfront.net/files/635...\n",
              "...         ...  ...                                                ...\n",
              "1713    7311793  ...  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "1714    7310878  ...  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "1715    7310802  ...  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "1716    7311831  ...  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "1717    7309810  ...  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "\n",
              "[1718 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAStIi8JNeZm",
        "outputId": "49fe3bc9-45d1-4d0e-e915-7089face2883"
      },
      "source": [
        "short_list_1500=[]\n",
        "for i in range(len(df_csv_joined['Text'])):\n",
        "    length = len(df_csv_joined['Text'][i])\n",
        "    if length<2000:\n",
        "      short_list_1500.append(i)\n",
        "\n",
        "len(short_list_1500) "
      ],
      "id": "ZAStIi8JNeZm",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "115"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 966
        },
        "id": "KQ_ImSbQat09",
        "outputId": "f6350186-9191-43d6-f3dd-2a551c3df37d"
      },
      "source": [
        "df_csv_joined1 = df_csv_joined.drop(short_list_1500)\n",
        "df_csv_joined1"
      ],
      "id": "KQ_ImSbQat09",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Answer ID</th>\n",
              "      <th>Answer Page</th>\n",
              "      <th>Metric</th>\n",
              "      <th>Company</th>\n",
              "      <th>Value</th>\n",
              "      <th>Source Page</th>\n",
              "      <th>Text</th>\n",
              "      <th>URL_link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6382834</td>\n",
              "      <td>https://wikirate.org/~6382834</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>S A Brain Company Ltd</td>\n",
              "      <td>1</td>\n",
              "      <td>https://wikirate.org/~6375302</td>\n",
              "      <td>10/19/2020\\n\\nSlavery and Human trafﬁcking | S...</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/637...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6368402</td>\n",
              "      <td>https://wikirate.org/~6368402</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Pension Protection Fund</td>\n",
              "      <td>1</td>\n",
              "      <td>https://wikirate.org/~6355100</td>\n",
              "      <td>\\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nModern Slav...</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/635...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6380781</td>\n",
              "      <td>https://wikirate.org/~6380781</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Hall &amp; Woodhouse Limited</td>\n",
              "      <td>1</td>\n",
              "      <td>https://wikirate.org/~6375083</td>\n",
              "      <td>10/19/2020\\n\\nSlavery | Hall and Woodhouse\\n\\n...</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/637...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6364508</td>\n",
              "      <td>https://wikirate.org/~6364508</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Greene King</td>\n",
              "      <td>1</td>\n",
              "      <td>https://wikirate.org/~6357513</td>\n",
              "      <td>Greene King | MODERN SLAVERY STATEMENT\\n\\nhttp...</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/635...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6379525</td>\n",
              "      <td>https://wikirate.org/~6379525</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Charles Wells</td>\n",
              "      <td>1</td>\n",
              "      <td>https://wikirate.org/~6354153</td>\n",
              "      <td>Charles Wells Ltd \\n \\nModern Slavery and Huma...</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/635...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1713</th>\n",
              "      <td>7311793</td>\n",
              "      <td>https://wikirate.org/~7311793</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>ECO Animal Health Group Plc</td>\n",
              "      <td>0</td>\n",
              "      <td>https://wikirate.org/~7307585</td>\n",
              "      <td>\\n\\n \\nModern Slavery Act 2015 Statement   \\n...</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1714</th>\n",
              "      <td>7310878</td>\n",
              "      <td>https://wikirate.org/~7310878</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Victoria Plum Limited</td>\n",
              "      <td>0</td>\n",
              "      <td>https://wikirate.org/~7307593</td>\n",
              "      <td>Upto 60% off + 20% off ends\\n\\n \\n\\n \\n\\nSearc...</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1715</th>\n",
              "      <td>7310802</td>\n",
              "      <td>https://wikirate.org/~7310802</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Creagh Concrete Products Limited</td>\n",
              "      <td>1</td>\n",
              "      <td>https://wikirate.org/~7307608</td>\n",
              "      <td>\\n\\nSlavery and Human Trafficking Transparenc...</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1716</th>\n",
              "      <td>7311831</td>\n",
              "      <td>https://wikirate.org/~7311831</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Reward Gateway (UK) Ltd</td>\n",
              "      <td>1</td>\n",
              "      <td>https://wikirate.org/~7307616</td>\n",
              "      <td>Reward Gateway Slavery\\nand Human Trafficking\\...</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1717</th>\n",
              "      <td>7309810</td>\n",
              "      <td>https://wikirate.org/~7309810</td>\n",
              "      <td>Walk Free Foundation+MSA whistleblowing mechan...</td>\n",
              "      <td>Origin Fertilisers</td>\n",
              "      <td>0</td>\n",
              "      <td>https://wikirate.org/~7307078</td>\n",
              "      <td>Modern Slavery Act Statement\\n– January 2021\\n...</td>\n",
              "      <td>https://dq06ugkuram52.cloudfront.net/files/730...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1603 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Answer ID  ...                                           URL_link\n",
              "0       6382834  ...  https://dq06ugkuram52.cloudfront.net/files/637...\n",
              "1       6368402  ...  https://dq06ugkuram52.cloudfront.net/files/635...\n",
              "2       6380781  ...  https://dq06ugkuram52.cloudfront.net/files/637...\n",
              "3       6364508  ...  https://dq06ugkuram52.cloudfront.net/files/635...\n",
              "4       6379525  ...  https://dq06ugkuram52.cloudfront.net/files/635...\n",
              "...         ...  ...                                                ...\n",
              "1713    7311793  ...  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "1714    7310878  ...  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "1715    7310802  ...  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "1716    7311831  ...  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "1717    7309810  ...  https://dq06ugkuram52.cloudfront.net/files/730...\n",
              "\n",
              "[1603 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf50c1ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0336470a-badb-4d1f-f9cd-6c4f03d177cd"
      },
      "source": [
        "stem_sentence_corpus = torch.load('drive/MyDrive/MSR_data/stem_sentence_corpus')\n",
        "\n",
        "stem_sentence_corpus_pd = pd.Series(stem_sentence_corpus)\n",
        "stem_sentence_corpus_pd"
      ],
      "id": "cf50c1ad",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [10/19/2020 slaveri and human trafﬁck | s.a., ...\n",
              "1       [modern slaveri act statement updated: septemb...\n",
              "2       [10/19/2020 slaveri | hall and woodhous modern...\n",
              "3       [green king | modern slaveri statement https:/...\n",
              "4       [charl well ltd modern slaveri and human traff...\n",
              "                              ...                        \n",
              "1713    [modern slaveri act 2015 statement our polici ...\n",
              "1714    [upto 60% off + 20% off end search for product...\n",
              "1715    [slaveri and human traffick transpar statement...\n",
              "1716    [reward gateway slaveri and human traffick sta...\n",
              "1717    [modern slaveri act statement – januari 2021 o...\n",
              "Length: 1718, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1azL9vU_TOhl",
        "outputId": "8e0fb09d-dbfa-4681-f352-e5b81ae219f7"
      },
      "source": [
        "stem_sentence_corpus_pd.drop(short_list_1500)"
      ],
      "id": "1azL9vU_TOhl",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [10/19/2020 slaveri and human trafﬁck | s.a., ...\n",
              "1       [modern slaveri act statement updated: septemb...\n",
              "2       [10/19/2020 slaveri | hall and woodhous modern...\n",
              "3       [green king | modern slaveri statement https:/...\n",
              "4       [charl well ltd modern slaveri and human traff...\n",
              "                              ...                        \n",
              "1713    [modern slaveri act 2015 statement our polici ...\n",
              "1714    [upto 60% off + 20% off end search for product...\n",
              "1715    [slaveri and human traffick transpar statement...\n",
              "1716    [reward gateway slaveri and human traffick sta...\n",
              "1717    [modern slaveri act statement – januari 2021 o...\n",
              "Length: 1603, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37ed49be"
      },
      "source": [
        "data_tensor = torch.load('drive/MyDrive/MSR_data/data_tensor_fnl')\n",
        "data_tensor_np = data_tensor.detach().numpy()"
      ],
      "id": "37ed49be",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uo8DrG1URn8h",
        "outputId": "ab212d97-3169-49e1-f81e-2886ec547ea8"
      },
      "source": [
        "data_tensor_np1 = np.delete(data_tensor_np, short_list_1500, axis=0)\n",
        "data_tensor_np1.shape"
      ],
      "id": "uo8DrG1URn8h",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1603, 200, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-gNMDPYaVct",
        "outputId": "6aedbfe2-a958-4b8b-c9de-229fdee69181"
      },
      "source": [
        "data_tensor2 = torch.tensor(data_tensor_np1)\n",
        "data_tensor2.size()"
      ],
      "id": "3-gNMDPYaVct",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1603, 200, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4d21773"
      },
      "source": [
        "#### I Tensor"
      ],
      "id": "b4d21773"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a9870c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "244fe573-4bb6-44dc-f2f5-1f9c88ca2f0f"
      },
      "source": [
        "I_tensor1 = torch.load('drive/MyDrive/MSR_data/I_tensor')\n",
        "I_tensor1_np = I_tensor1.detach().numpy()\n",
        "I_tensor1_np1 = np.delete(I_tensor1_np, short_list_1500, axis=0)\n",
        "I_tensor1_np1.shape"
      ],
      "id": "2a9870c1",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1603, 1, 200)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O62Bacf-Zwcm",
        "outputId": "737f57b8-a75e-4e24-cf53-5003a04f31c8"
      },
      "source": [
        "I_tensor2 = torch.tensor(I_tensor1_np1)\n",
        "I_tensor2.size()"
      ],
      "id": "O62Bacf-Zwcm",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1603, 1, 200])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50b22995"
      },
      "source": [
        "### Model training"
      ],
      "id": "50b22995"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbf0497f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf50e66e-d27e-4c1d-d7c7-3e2f55543b22"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "id": "fbf0497f",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gfTDR1LcWyB",
        "outputId": "772a360d-5836-4447-f0a6-f5c8d11c05d3"
      },
      "source": [
        "label = df_csv_joined1['Value'].tolist()\n",
        "len(label)"
      ],
      "id": "1gfTDR1LcWyB",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1603"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a98c0ee3"
      },
      "source": [
        "label = df_csv_joined1['Value'].tolist()\n",
        "\n",
        "train_x = data_tensor2[:1200]\n",
        "train_x = train_x.permute(0,2,1)\n",
        "train_y = torch.tensor(label[:1200])\n",
        "train_i = I_tensor2[:1200]\n",
        "\n",
        "val_x = data_tensor2[1200:]\n",
        "val_x = val_x.permute(0,2,1)\n",
        "val_y = torch.tensor(label[1200:])\n",
        "val_i = I_tensor2[1200:]\n",
        "\n",
        "test_x = data_tensor2[1200:]\n",
        "test_x = test_x.permute(0,2,1)\n",
        "test_y = torch.tensor(label[1200:])\n",
        "test_i = I_tensor2[1200:]"
      ],
      "id": "a98c0ee3",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ea39972"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "train_data = TensorDataset(train_x, train_y, train_i)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "val_data = TensorDataset(val_x, val_y, val_i)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size, shuffle=False)"
      ],
      "id": "7ea39972",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be4f5263"
      },
      "source": [
        "class model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        \n",
        "        super(model, self).__init__()\n",
        "                        \n",
        "        self.conv1 = nn.Conv1d(in_channels = 768, out_channels=1, kernel_size=1, stride=1, padding=0)\n",
        "        \n",
        "        self.conv2 = nn.Conv1d(in_channels = 768, out_channels=1024, kernel_size=1, stride=1, padding=0)\n",
        "        \n",
        "        self.fc1 = nn.Linear(1024, 100)\n",
        "        \n",
        "        self.fc2 = nn.Linear(100, 2)\n",
        "                \n",
        "        self.softmax1 = nn.Softmax(dim=-1)\n",
        "        \n",
        "        #self.softmax2 = nn.Softmax(dim=1)\n",
        "        \n",
        "        self.relu =  nn.ReLU()\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, x, y, i):\n",
        "        \n",
        "        lbda = 25\n",
        "                        \n",
        "        alpha = self.conv1(x) #(btch_sz, 768, 200)-> (btch_sz, 1, 200)\n",
        "        \n",
        "        x_2 = self.conv2(x)\n",
        "        \n",
        "        x_2 = self.relu(x_2)\n",
        "                                                        \n",
        "        alpha1 = alpha + lbda * i #(btch_sz, 1, 200) + (btch_sz,1, 200)*(btch_sz, 1, 200) \n",
        "                                \n",
        "        alpha_prime = self.softmax1(alpha1) #(btch_sz, 1, 200)\n",
        "                        \n",
        "        alpha_prime_permute = alpha_prime.permute(0,2,1) #(btch_sz, 200, 1) \n",
        "                                        \n",
        "        h = torch.matmul(x_2, alpha_prime_permute)  #(btch_sz, 768, 200)*(btch_sz, 200, 1) = (btch_sz, 768, 1)\n",
        "                                \n",
        "        h = h.view(h.shape[0], -1) #(btch_sz, 768)\n",
        "                        \n",
        "        output = self.relu(self.fc1(h)) #(btch_sz, 768, 512) \n",
        "        \n",
        "        output = self.dropout(output)\n",
        "        \n",
        "        output = self.fc2(output)\n",
        "        \n",
        "        #output = self.softmax2(output)\n",
        "                                 \n",
        "        return output"
      ],
      "id": "be4f5263",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23296af6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23e8b486-b6d3-4041-e497-59efce4348e0"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        " \n",
        "class_wts = compute_class_weight('balanced', classes= torch.unique(train_y).tolist(), y=train_y.tolist())\n",
        "\n",
        "print(class_wts)\n",
        "\n",
        "weights = torch.tensor(class_wts, dtype = torch.float)\n",
        "weights = weights.to(device)\n"
      ],
      "id": "23296af6",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.05820106 0.9478673 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9-CQ-RWP6kX"
      },
      "source": [
        "from torch import optim\n",
        "\n",
        "Model = model()\n",
        "\n",
        "Model = Model.to(device)\n",
        "\n",
        "cross_entropy = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "from transformers import AdamW\n",
        "\n",
        "optimizer = optim.AdamW(Model.parameters(), lr = 0.001, weight_decay=1e-5)\n",
        "\n",
        "epochs = 200\n"
      ],
      "id": "R9-CQ-RWP6kX",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f0f0d18"
      },
      "source": [
        "def train():\n",
        "    Model.train()\n",
        "    \n",
        "    total_loss = 0\n",
        "\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    \n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        batch = [r.to(device) for r in batch]\n",
        " \n",
        "        x, y, i = batch\n",
        "\n",
        "        Model.zero_grad()        \n",
        "\n",
        "        preds = Model(x, y, i)\n",
        "\n",
        "        loss = cross_entropy(preds, y)\n",
        "\n",
        "        total_loss = total_loss + loss.item()\n",
        "     \n",
        "        loss.backward()\n",
        "\n",
        "        #torch.nn.utils.clip_grad_norm_(Model.parameters(), 1.0)  # clip the the gradients to 1.0, preventing the exploding gradient problem\n",
        "\n",
        "        optimizer.step()  # update parameters\n",
        "\n",
        "        _, predicted = torch.max(preds.data, 1)\n",
        "        train_total += y.size(0)\n",
        "        train_correct += (predicted == y).sum().item()\n",
        "\n",
        "        preds=preds.detach().cpu().numpy()   # if model predictions are stored on GPU, push it to CPU\n",
        "        \n",
        "    avg_loss = total_loss / len(train_dataloader) # compute the training loss of the epoch\n",
        "    train_acc = train_correct/train_total\n",
        "  \n",
        "    return avg_loss, train_acc #returns train loss and predictions\n"
      ],
      "id": "6f0f0d18",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55179695"
      },
      "source": [
        "def evaluate():\n",
        "    \n",
        "    print(\"\\nEvaluating...\")\n",
        "    Model.eval()   # deactivate dropout layers\n",
        "    \n",
        "    total_loss = 0\n",
        "\n",
        "    val_total = 0\n",
        "    val_correct = 0\n",
        "\n",
        "    for step,batch in enumerate(val_dataloader):   # iterate over batches\n",
        "        \n",
        "        batch = [t.to(device) for t in batch] # push the batch to gpu\n",
        "\n",
        "        x, y, i = batch\n",
        "        \n",
        "        with torch.no_grad(): # deactivate autograd\n",
        "            \n",
        "            preds = Model(x, y, i)\n",
        "            \n",
        "            loss = cross_entropy(preds,y)\n",
        "\n",
        "            total_loss = total_loss + loss.item()\n",
        "\n",
        "            _, predicted = torch.max(preds.data, 1)\n",
        "            val_total += y.size(0)\n",
        "            val_correct += (predicted == y).sum().item()\n",
        "\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "            \n",
        "    avg_loss = total_loss / len(val_dataloader) \n",
        "    val_acc = val_correct/val_total\n",
        "\n",
        "    return avg_loss, val_acc"
      ],
      "id": "55179695",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f71e6bdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "335f43d7-c912-48a1-e1f9-f4794d025d01"
      },
      "source": [
        "import numpy as np\n",
        "best_valid_loss = float('inf')  # set initial loss to infinite\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "train_acces = []\n",
        "val_acces = []\n",
        "\n",
        "for epoch in range(epochs): #for each epoch\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "\n",
        "    train_loss, train_acc = train() #train model\n",
        "    valid_loss, val_acc = evaluate() #evaluate model\n",
        "    \n",
        "    if valid_loss < best_valid_loss: #save the best model\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(Model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    \n",
        "    train_acces.append(train_acc)\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    val_acces.append(val_acces)\n",
        "    \n",
        "    print('Train acc', np.round(train_acc, 3), 'Val acc', np.round(val_acc, 3))\n",
        "    print('Train loss', np.round(train_loss, 3), 'Val loss', np.round(valid_loss, 3))\n"
      ],
      "id": "f71e6bdb",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.624 Val acc 0.68\n",
            "Train loss 0.679 Val loss 0.641\n",
            "\n",
            " Epoch 2 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.649 Val acc 0.68\n",
            "Train loss 0.633 Val loss 0.651\n",
            "\n",
            " Epoch 3 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.673 Val acc 0.68\n",
            "Train loss 0.629 Val loss 0.642\n",
            "\n",
            " Epoch 4 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.668 Val acc 0.68\n",
            "Train loss 0.622 Val loss 0.645\n",
            "\n",
            " Epoch 5 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.672 Val acc 0.682\n",
            "Train loss 0.624 Val loss 0.646\n",
            "\n",
            " Epoch 6 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.666 Val acc 0.665\n",
            "Train loss 0.621 Val loss 0.651\n",
            "\n",
            " Epoch 7 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.681 Val acc 0.66\n",
            "Train loss 0.612 Val loss 0.647\n",
            "\n",
            " Epoch 8 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.674 Val acc 0.603\n",
            "Train loss 0.617 Val loss 0.668\n",
            "\n",
            " Epoch 9 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.668 Val acc 0.675\n",
            "Train loss 0.618 Val loss 0.646\n",
            "\n",
            " Epoch 10 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.677 Val acc 0.603\n",
            "Train loss 0.613 Val loss 0.672\n",
            "\n",
            " Epoch 11 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.648 Val acc 0.67\n",
            "Train loss 0.617 Val loss 0.643\n",
            "\n",
            " Epoch 12 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.696 Val acc 0.643\n",
            "Train loss 0.607 Val loss 0.65\n",
            "\n",
            " Epoch 13 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.688 Val acc 0.633\n",
            "Train loss 0.611 Val loss 0.661\n",
            "\n",
            " Epoch 14 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.68 Val acc 0.61\n",
            "Train loss 0.612 Val loss 0.665\n",
            "\n",
            " Epoch 15 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.674 Val acc 0.645\n",
            "Train loss 0.612 Val loss 0.65\n",
            "\n",
            " Epoch 16 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.692 Val acc 0.64\n",
            "Train loss 0.607 Val loss 0.659\n",
            "\n",
            " Epoch 17 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.694 Val acc 0.638\n",
            "Train loss 0.608 Val loss 0.661\n",
            "\n",
            " Epoch 18 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.675 Val acc 0.66\n",
            "Train loss 0.61 Val loss 0.645\n",
            "\n",
            " Epoch 19 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.682 Val acc 0.677\n",
            "Train loss 0.612 Val loss 0.642\n",
            "\n",
            " Epoch 20 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.691 Val acc 0.64\n",
            "Train loss 0.611 Val loss 0.652\n",
            "\n",
            " Epoch 21 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.694 Val acc 0.638\n",
            "Train loss 0.605 Val loss 0.653\n",
            "\n",
            " Epoch 22 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.69 Val acc 0.638\n",
            "Train loss 0.604 Val loss 0.664\n",
            "\n",
            " Epoch 23 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.679 Val acc 0.618\n",
            "Train loss 0.613 Val loss 0.668\n",
            "\n",
            " Epoch 24 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.693 Val acc 0.645\n",
            "Train loss 0.61 Val loss 0.649\n",
            "\n",
            " Epoch 25 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.696 Val acc 0.663\n",
            "Train loss 0.608 Val loss 0.646\n",
            "\n",
            " Epoch 26 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.681 Val acc 0.67\n",
            "Train loss 0.609 Val loss 0.643\n",
            "\n",
            " Epoch 27 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.687 Val acc 0.63\n",
            "Train loss 0.605 Val loss 0.658\n",
            "\n",
            " Epoch 28 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.685 Val acc 0.64\n",
            "Train loss 0.612 Val loss 0.662\n",
            "\n",
            " Epoch 29 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.684 Val acc 0.648\n",
            "Train loss 0.609 Val loss 0.651\n",
            "\n",
            " Epoch 30 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.691 Val acc 0.635\n",
            "Train loss 0.605 Val loss 0.664\n",
            "\n",
            " Epoch 31 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.685 Val acc 0.645\n",
            "Train loss 0.614 Val loss 0.657\n",
            "\n",
            " Epoch 32 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.688 Val acc 0.67\n",
            "Train loss 0.608 Val loss 0.644\n",
            "\n",
            " Epoch 33 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.691 Val acc 0.643\n",
            "Train loss 0.606 Val loss 0.646\n",
            "\n",
            " Epoch 34 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.686 Val acc 0.65\n",
            "Train loss 0.61 Val loss 0.652\n",
            "\n",
            " Epoch 35 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.683 Val acc 0.658\n",
            "Train loss 0.605 Val loss 0.645\n",
            "\n",
            " Epoch 36 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.692 Val acc 0.653\n",
            "Train loss 0.605 Val loss 0.645\n",
            "\n",
            " Epoch 37 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.684 Val acc 0.677\n",
            "Train loss 0.608 Val loss 0.641\n",
            "\n",
            " Epoch 38 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.69 Val acc 0.65\n",
            "Train loss 0.604 Val loss 0.652\n",
            "\n",
            " Epoch 39 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.687 Val acc 0.645\n",
            "Train loss 0.606 Val loss 0.654\n",
            "\n",
            " Epoch 40 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.689 Val acc 0.635\n",
            "Train loss 0.607 Val loss 0.664\n",
            "\n",
            " Epoch 41 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.69 Val acc 0.653\n",
            "Train loss 0.606 Val loss 0.651\n",
            "\n",
            " Epoch 42 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.697 Val acc 0.67\n",
            "Train loss 0.607 Val loss 0.645\n",
            "\n",
            " Epoch 43 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.697 Val acc 0.675\n",
            "Train loss 0.606 Val loss 0.639\n",
            "\n",
            " Epoch 44 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.686 Val acc 0.67\n",
            "Train loss 0.613 Val loss 0.641\n",
            "\n",
            " Epoch 45 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.688 Val acc 0.67\n",
            "Train loss 0.608 Val loss 0.644\n",
            "\n",
            " Epoch 46 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.694 Val acc 0.65\n",
            "Train loss 0.606 Val loss 0.654\n",
            "\n",
            " Epoch 47 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.694 Val acc 0.645\n",
            "Train loss 0.605 Val loss 0.649\n",
            "\n",
            " Epoch 48 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.695 Val acc 0.63\n",
            "Train loss 0.603 Val loss 0.666\n",
            "\n",
            " Epoch 49 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.693 Val acc 0.655\n",
            "Train loss 0.604 Val loss 0.645\n",
            "\n",
            " Epoch 50 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.692 Val acc 0.62\n",
            "Train loss 0.606 Val loss 0.669\n",
            "\n",
            " Epoch 51 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.692 Val acc 0.655\n",
            "Train loss 0.606 Val loss 0.645\n",
            "\n",
            " Epoch 52 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.689 Val acc 0.638\n",
            "Train loss 0.604 Val loss 0.666\n",
            "\n",
            " Epoch 53 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.692 Val acc 0.653\n",
            "Train loss 0.604 Val loss 0.651\n",
            "\n",
            " Epoch 54 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.693 Val acc 0.667\n",
            "Train loss 0.606 Val loss 0.642\n",
            "\n",
            " Epoch 55 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.688 Val acc 0.645\n",
            "Train loss 0.603 Val loss 0.65\n",
            "\n",
            " Epoch 56 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.685 Val acc 0.645\n",
            "Train loss 0.605 Val loss 0.651\n",
            "\n",
            " Epoch 57 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.698 Val acc 0.63\n",
            "Train loss 0.604 Val loss 0.664\n",
            "\n",
            " Epoch 58 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.688 Val acc 0.638\n",
            "Train loss 0.606 Val loss 0.659\n",
            "\n",
            " Epoch 59 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.692 Val acc 0.623\n",
            "Train loss 0.605 Val loss 0.663\n",
            "\n",
            " Epoch 60 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.695 Val acc 0.63\n",
            "Train loss 0.605 Val loss 0.665\n",
            "\n",
            " Epoch 61 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.69 Val acc 0.615\n",
            "Train loss 0.605 Val loss 0.669\n",
            "\n",
            " Epoch 62 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.691 Val acc 0.63\n",
            "Train loss 0.603 Val loss 0.658\n",
            "\n",
            " Epoch 63 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.69 Val acc 0.63\n",
            "Train loss 0.607 Val loss 0.658\n",
            "\n",
            " Epoch 64 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.692 Val acc 0.658\n",
            "Train loss 0.606 Val loss 0.646\n",
            "\n",
            " Epoch 65 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.687 Val acc 0.672\n",
            "Train loss 0.606 Val loss 0.64\n",
            "\n",
            " Epoch 66 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.692 Val acc 0.65\n",
            "Train loss 0.605 Val loss 0.654\n",
            "\n",
            " Epoch 67 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.694 Val acc 0.67\n",
            "Train loss 0.602 Val loss 0.64\n",
            "\n",
            " Epoch 68 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.7 Val acc 0.66\n",
            "Train loss 0.609 Val loss 0.65\n",
            "\n",
            " Epoch 69 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.691 Val acc 0.653\n",
            "Train loss 0.602 Val loss 0.647\n",
            "\n",
            " Epoch 70 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.701 Val acc 0.623\n",
            "Train loss 0.601 Val loss 0.676\n",
            "\n",
            " Epoch 71 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.683 Val acc 0.633\n",
            "Train loss 0.605 Val loss 0.66\n",
            "\n",
            " Epoch 72 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.687 Val acc 0.682\n",
            "Train loss 0.603 Val loss 0.642\n",
            "\n",
            " Epoch 73 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.692 Val acc 0.658\n",
            "Train loss 0.605 Val loss 0.646\n",
            "\n",
            " Epoch 74 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.693 Val acc 0.658\n",
            "Train loss 0.607 Val loss 0.649\n",
            "\n",
            " Epoch 75 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.694 Val acc 0.655\n",
            "Train loss 0.602 Val loss 0.648\n",
            "\n",
            " Epoch 76 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.694 Val acc 0.635\n",
            "Train loss 0.601 Val loss 0.658\n",
            "\n",
            " Epoch 77 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.69 Val acc 0.667\n",
            "Train loss 0.6 Val loss 0.642\n",
            "\n",
            " Epoch 78 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.695 Val acc 0.645\n",
            "Train loss 0.604 Val loss 0.651\n",
            "\n",
            " Epoch 79 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.691 Val acc 0.658\n",
            "Train loss 0.602 Val loss 0.647\n",
            "\n",
            " Epoch 80 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.693 Val acc 0.658\n",
            "Train loss 0.602 Val loss 0.645\n",
            "\n",
            " Epoch 81 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.693 Val acc 0.635\n",
            "Train loss 0.601 Val loss 0.663\n",
            "\n",
            " Epoch 82 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.688 Val acc 0.648\n",
            "Train loss 0.606 Val loss 0.651\n",
            "\n",
            " Epoch 83 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.692 Val acc 0.653\n",
            "Train loss 0.601 Val loss 0.651\n",
            "\n",
            " Epoch 84 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.695 Val acc 0.648\n",
            "Train loss 0.602 Val loss 0.655\n",
            "\n",
            " Epoch 85 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.692 Val acc 0.667\n",
            "Train loss 0.601 Val loss 0.645\n",
            "\n",
            " Epoch 86 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.692 Val acc 0.65\n",
            "Train loss 0.605 Val loss 0.653\n",
            "\n",
            " Epoch 87 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.697 Val acc 0.638\n",
            "Train loss 0.601 Val loss 0.656\n",
            "\n",
            " Epoch 88 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.693 Val acc 0.658\n",
            "Train loss 0.602 Val loss 0.646\n",
            "\n",
            " Epoch 89 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.694 Val acc 0.638\n",
            "Train loss 0.6 Val loss 0.66\n",
            "\n",
            " Epoch 90 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.697 Val acc 0.643\n",
            "Train loss 0.599 Val loss 0.661\n",
            "\n",
            " Epoch 91 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.693 Val acc 0.648\n",
            "Train loss 0.6 Val loss 0.652\n",
            "\n",
            " Epoch 92 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.697 Val acc 0.633\n",
            "Train loss 0.603 Val loss 0.657\n",
            "\n",
            " Epoch 93 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.688 Val acc 0.635\n",
            "Train loss 0.606 Val loss 0.657\n",
            "\n",
            " Epoch 94 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.682 Val acc 0.655\n",
            "Train loss 0.607 Val loss 0.65\n",
            "\n",
            " Epoch 95 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.698 Val acc 0.638\n",
            "Train loss 0.604 Val loss 0.653\n",
            "\n",
            " Epoch 96 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.696 Val acc 0.623\n",
            "Train loss 0.604 Val loss 0.675\n",
            "\n",
            " Epoch 97 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.688 Val acc 0.66\n",
            "Train loss 0.605 Val loss 0.652\n",
            "\n",
            " Epoch 98 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.693 Val acc 0.675\n",
            "Train loss 0.602 Val loss 0.638\n",
            "\n",
            " Epoch 99 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.694 Val acc 0.623\n",
            "Train loss 0.599 Val loss 0.668\n",
            "\n",
            " Epoch 100 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.695 Val acc 0.658\n",
            "Train loss 0.604 Val loss 0.647\n",
            "\n",
            " Epoch 101 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.698 Val acc 0.635\n",
            "Train loss 0.603 Val loss 0.657\n",
            "\n",
            " Epoch 102 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.697 Val acc 0.635\n",
            "Train loss 0.6 Val loss 0.666\n",
            "\n",
            " Epoch 103 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.692 Val acc 0.66\n",
            "Train loss 0.598 Val loss 0.648\n",
            "\n",
            " Epoch 104 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.694 Val acc 0.645\n",
            "Train loss 0.603 Val loss 0.654\n",
            "\n",
            " Epoch 105 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.691 Val acc 0.66\n",
            "Train loss 0.602 Val loss 0.645\n",
            "\n",
            " Epoch 106 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.699 Val acc 0.663\n",
            "Train loss 0.604 Val loss 0.647\n",
            "\n",
            " Epoch 107 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.692 Val acc 0.658\n",
            "Train loss 0.6 Val loss 0.649\n",
            "\n",
            " Epoch 108 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.695 Val acc 0.645\n",
            "Train loss 0.599 Val loss 0.654\n",
            "\n",
            " Epoch 109 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.698 Val acc 0.645\n",
            "Train loss 0.597 Val loss 0.654\n",
            "\n",
            " Epoch 110 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.697 Val acc 0.667\n",
            "Train loss 0.599 Val loss 0.64\n",
            "\n",
            " Epoch 111 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.697 Val acc 0.64\n",
            "Train loss 0.598 Val loss 0.658\n",
            "\n",
            " Epoch 112 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.695 Val acc 0.623\n",
            "Train loss 0.6 Val loss 0.674\n",
            "\n",
            " Epoch 113 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.687 Val acc 0.667\n",
            "Train loss 0.601 Val loss 0.639\n",
            "\n",
            " Epoch 114 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.695 Val acc 0.648\n",
            "Train loss 0.596 Val loss 0.656\n",
            "\n",
            " Epoch 115 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.693 Val acc 0.655\n",
            "Train loss 0.596 Val loss 0.65\n",
            "\n",
            " Epoch 116 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.697 Val acc 0.628\n",
            "Train loss 0.596 Val loss 0.666\n",
            "\n",
            " Epoch 117 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.697 Val acc 0.653\n",
            "Train loss 0.596 Val loss 0.646\n",
            "\n",
            " Epoch 118 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.697 Val acc 0.633\n",
            "Train loss 0.597 Val loss 0.668\n",
            "\n",
            " Epoch 119 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.685 Val acc 0.655\n",
            "Train loss 0.604 Val loss 0.651\n",
            "\n",
            " Epoch 120 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.692 Val acc 0.65\n",
            "Train loss 0.599 Val loss 0.648\n",
            "\n",
            " Epoch 121 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.693 Val acc 0.658\n",
            "Train loss 0.6 Val loss 0.644\n",
            "\n",
            " Epoch 122 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.695 Val acc 0.658\n",
            "Train loss 0.597 Val loss 0.654\n",
            "\n",
            " Epoch 123 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.692 Val acc 0.655\n",
            "Train loss 0.597 Val loss 0.65\n",
            "\n",
            " Epoch 124 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.702 Val acc 0.655\n",
            "Train loss 0.594 Val loss 0.654\n",
            "\n",
            " Epoch 125 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.697 Val acc 0.663\n",
            "Train loss 0.596 Val loss 0.648\n",
            "\n",
            " Epoch 126 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.699 Val acc 0.65\n",
            "Train loss 0.597 Val loss 0.651\n",
            "\n",
            " Epoch 127 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.698 Val acc 0.653\n",
            "Train loss 0.6 Val loss 0.656\n",
            "\n",
            " Epoch 128 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.695 Val acc 0.663\n",
            "Train loss 0.594 Val loss 0.646\n",
            "\n",
            " Epoch 129 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.694 Val acc 0.663\n",
            "Train loss 0.598 Val loss 0.648\n",
            "\n",
            " Epoch 130 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.696 Val acc 0.66\n",
            "Train loss 0.592 Val loss 0.653\n",
            "\n",
            " Epoch 131 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.698 Val acc 0.635\n",
            "Train loss 0.596 Val loss 0.666\n",
            "\n",
            " Epoch 132 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.702 Val acc 0.655\n",
            "Train loss 0.598 Val loss 0.653\n",
            "\n",
            " Epoch 133 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.703 Val acc 0.655\n",
            "Train loss 0.594 Val loss 0.655\n",
            "\n",
            " Epoch 134 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.699 Val acc 0.665\n",
            "Train loss 0.592 Val loss 0.643\n",
            "\n",
            " Epoch 135 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.685 Val acc 0.655\n",
            "Train loss 0.596 Val loss 0.651\n",
            "\n",
            " Epoch 136 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.698 Val acc 0.64\n",
            "Train loss 0.593 Val loss 0.657\n",
            "\n",
            " Epoch 137 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.703 Val acc 0.653\n",
            "Train loss 0.593 Val loss 0.652\n",
            "\n",
            " Epoch 138 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.699 Val acc 0.663\n",
            "Train loss 0.595 Val loss 0.645\n",
            "\n",
            " Epoch 139 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.698 Val acc 0.655\n",
            "Train loss 0.597 Val loss 0.652\n",
            "\n",
            " Epoch 140 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.702 Val acc 0.655\n",
            "Train loss 0.596 Val loss 0.65\n",
            "\n",
            " Epoch 141 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.7 Val acc 0.663\n",
            "Train loss 0.591 Val loss 0.65\n",
            "\n",
            " Epoch 142 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.696 Val acc 0.625\n",
            "Train loss 0.594 Val loss 0.679\n",
            "\n",
            " Epoch 143 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.708 Val acc 0.633\n",
            "Train loss 0.592 Val loss 0.666\n",
            "\n",
            " Epoch 144 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.696 Val acc 0.658\n",
            "Train loss 0.595 Val loss 0.658\n",
            "\n",
            " Epoch 145 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.699 Val acc 0.663\n",
            "Train loss 0.594 Val loss 0.65\n",
            "\n",
            " Epoch 146 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.699 Val acc 0.655\n",
            "Train loss 0.594 Val loss 0.653\n",
            "\n",
            " Epoch 147 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.7 Val acc 0.551\n",
            "Train loss 0.586 Val loss 0.72\n",
            "\n",
            " Epoch 148 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.696 Val acc 0.67\n",
            "Train loss 0.598 Val loss 0.644\n",
            "\n",
            " Epoch 149 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.702 Val acc 0.658\n",
            "Train loss 0.597 Val loss 0.659\n",
            "\n",
            " Epoch 150 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.698 Val acc 0.638\n",
            "Train loss 0.598 Val loss 0.656\n",
            "\n",
            " Epoch 151 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.699 Val acc 0.635\n",
            "Train loss 0.598 Val loss 0.669\n",
            "\n",
            " Epoch 152 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.699 Val acc 0.658\n",
            "Train loss 0.594 Val loss 0.649\n",
            "\n",
            " Epoch 153 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.701 Val acc 0.658\n",
            "Train loss 0.594 Val loss 0.656\n",
            "\n",
            " Epoch 154 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.699 Val acc 0.63\n",
            "Train loss 0.595 Val loss 0.672\n",
            "\n",
            " Epoch 155 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.698 Val acc 0.635\n",
            "Train loss 0.593 Val loss 0.662\n",
            "\n",
            " Epoch 156 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.697 Val acc 0.663\n",
            "Train loss 0.594 Val loss 0.651\n",
            "\n",
            " Epoch 157 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.695 Val acc 0.663\n",
            "Train loss 0.595 Val loss 0.646\n",
            "\n",
            " Epoch 158 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.701 Val acc 0.63\n",
            "Train loss 0.591 Val loss 0.673\n",
            "\n",
            " Epoch 159 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.697 Val acc 0.66\n",
            "Train loss 0.591 Val loss 0.643\n",
            "\n",
            " Epoch 160 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.702 Val acc 0.633\n",
            "Train loss 0.593 Val loss 0.684\n",
            "\n",
            " Epoch 161 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.703 Val acc 0.633\n",
            "Train loss 0.597 Val loss 0.669\n",
            "\n",
            " Epoch 162 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.698 Val acc 0.635\n",
            "Train loss 0.594 Val loss 0.663\n",
            "\n",
            " Epoch 163 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.702 Val acc 0.658\n",
            "Train loss 0.59 Val loss 0.656\n",
            "\n",
            " Epoch 164 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.702 Val acc 0.635\n",
            "Train loss 0.587 Val loss 0.671\n",
            "\n",
            " Epoch 165 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.708 Val acc 0.655\n",
            "Train loss 0.592 Val loss 0.655\n",
            "\n",
            " Epoch 166 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.702 Val acc 0.66\n",
            "Train loss 0.593 Val loss 0.658\n",
            "\n",
            " Epoch 167 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.698 Val acc 0.63\n",
            "Train loss 0.586 Val loss 0.663\n",
            "\n",
            " Epoch 168 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.696 Val acc 0.628\n",
            "Train loss 0.594 Val loss 0.67\n",
            "\n",
            " Epoch 169 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.704 Val acc 0.66\n",
            "Train loss 0.592 Val loss 0.651\n",
            "\n",
            " Epoch 170 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.703 Val acc 0.625\n",
            "Train loss 0.596 Val loss 0.674\n",
            "\n",
            " Epoch 171 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.705 Val acc 0.66\n",
            "Train loss 0.591 Val loss 0.648\n",
            "\n",
            " Epoch 172 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.698 Val acc 0.633\n",
            "Train loss 0.586 Val loss 0.672\n",
            "\n",
            " Epoch 173 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.698 Val acc 0.658\n",
            "Train loss 0.586 Val loss 0.653\n",
            "\n",
            " Epoch 174 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.708 Val acc 0.625\n",
            "Train loss 0.588 Val loss 0.676\n",
            "\n",
            " Epoch 175 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.707 Val acc 0.65\n",
            "Train loss 0.585 Val loss 0.653\n",
            "\n",
            " Epoch 176 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.704 Val acc 0.66\n",
            "Train loss 0.586 Val loss 0.66\n",
            "\n",
            " Epoch 177 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.708 Val acc 0.658\n",
            "Train loss 0.589 Val loss 0.648\n",
            "\n",
            " Epoch 178 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.71 Val acc 0.633\n",
            "Train loss 0.582 Val loss 0.678\n",
            "\n",
            " Epoch 179 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.693 Val acc 0.63\n",
            "Train loss 0.594 Val loss 0.66\n",
            "\n",
            " Epoch 180 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.703 Val acc 0.628\n",
            "Train loss 0.588 Val loss 0.665\n",
            "\n",
            " Epoch 181 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.698 Val acc 0.63\n",
            "Train loss 0.591 Val loss 0.675\n",
            "\n",
            " Epoch 182 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.704 Val acc 0.638\n",
            "Train loss 0.591 Val loss 0.666\n",
            "\n",
            " Epoch 183 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.703 Val acc 0.648\n",
            "Train loss 0.589 Val loss 0.663\n",
            "\n",
            " Epoch 184 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.7 Val acc 0.653\n",
            "Train loss 0.595 Val loss 0.658\n",
            "\n",
            " Epoch 185 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.699 Val acc 0.66\n",
            "Train loss 0.586 Val loss 0.652\n",
            "\n",
            " Epoch 186 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.698 Val acc 0.643\n",
            "Train loss 0.587 Val loss 0.666\n",
            "\n",
            " Epoch 187 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.704 Val acc 0.655\n",
            "Train loss 0.584 Val loss 0.657\n",
            "\n",
            " Epoch 188 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.704 Val acc 0.63\n",
            "Train loss 0.585 Val loss 0.665\n",
            "\n",
            " Epoch 189 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.706 Val acc 0.643\n",
            "Train loss 0.585 Val loss 0.668\n",
            "\n",
            " Epoch 190 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.699 Val acc 0.635\n",
            "Train loss 0.589 Val loss 0.679\n",
            "\n",
            " Epoch 191 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.706 Val acc 0.655\n",
            "Train loss 0.583 Val loss 0.651\n",
            "\n",
            " Epoch 192 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.702 Val acc 0.628\n",
            "Train loss 0.58 Val loss 0.678\n",
            "\n",
            " Epoch 193 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.704 Val acc 0.648\n",
            "Train loss 0.581 Val loss 0.657\n",
            "\n",
            " Epoch 194 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.698 Val acc 0.638\n",
            "Train loss 0.589 Val loss 0.657\n",
            "\n",
            " Epoch 195 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.705 Val acc 0.635\n",
            "Train loss 0.587 Val loss 0.666\n",
            "\n",
            " Epoch 196 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.708 Val acc 0.625\n",
            "Train loss 0.58 Val loss 0.671\n",
            "\n",
            " Epoch 197 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.704 Val acc 0.66\n",
            "Train loss 0.583 Val loss 0.654\n",
            "\n",
            " Epoch 198 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.7 Val acc 0.635\n",
            "Train loss 0.585 Val loss 0.669\n",
            "\n",
            " Epoch 199 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.702 Val acc 0.653\n",
            "Train loss 0.587 Val loss 0.658\n",
            "\n",
            " Epoch 200 / 200\n",
            "\n",
            "Evaluating...\n",
            "Train acc 0.708 Val acc 0.658\n",
            "Train loss 0.588 Val loss 0.658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNS0HjWAKMc1",
        "outputId": "e8873966-ca62-4d93-b5f1-5a9f040758d1"
      },
      "source": [
        "path = 'saved_weights.pt'\n",
        "Model.load_state_dict(torch.load(path))\n",
        "\n",
        "print('Train_report:')\n",
        "with torch.no_grad():\n",
        "  train_preds = Model(train_x.to(device), train_y.to(device), train_i.to(device))\n",
        "  train_preds = train_preds.detach().cpu().numpy()\n",
        "\n",
        "train_preds = np.argmax(train_preds, axis = 1)\n",
        "print(classification_report(train_y, train_preds))\n",
        "\n",
        "\n",
        "print('Val_report:')\n",
        "with torch.no_grad():\n",
        "  val_preds = Model(val_x.to(device), val_y.to(device), val_i.to(device))\n",
        "  val_preds = val_preds.detach().cpu().numpy()\n",
        "\n",
        "val_preds = np.argmax(val_preds, axis = 1)\n",
        "print(classification_report(val_y, val_preds))\n",
        "\n",
        "print('Test_report:')\n",
        "with torch.no_grad():\n",
        "  test_preds = Model(test_x.to(device), test_y.to(device), test_i.to(device))\n",
        "  test_preds = test_preds.detach().cpu().numpy()\n",
        "\n",
        "test_preds = np.argmax(test_preds, axis = 1)\n",
        "print(classification_report(test_y, test_preds))"
      ],
      "id": "rNS0HjWAKMc1",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train_report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.50      0.61       567\n",
            "           1       0.66      0.86      0.75       633\n",
            "\n",
            "    accuracy                           0.69      1200\n",
            "   macro avg       0.71      0.68      0.68      1200\n",
            "weighted avg       0.71      0.69      0.68      1200\n",
            "\n",
            "Val_report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.31      0.42       150\n",
            "           1       0.69      0.89      0.77       253\n",
            "\n",
            "    accuracy                           0.67       403\n",
            "   macro avg       0.66      0.60      0.60       403\n",
            "weighted avg       0.66      0.67      0.64       403\n",
            "\n",
            "Test_report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.31      0.42       150\n",
            "           1       0.69      0.89      0.77       253\n",
            "\n",
            "    accuracy                           0.67       403\n",
            "   macro avg       0.66      0.60      0.60       403\n",
            "weighted avg       0.66      0.67      0.64       403\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K0LikqKs4v3"
      },
      "source": [
        "x = list(range(0,200))\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(x, train_acces)\n",
        "plt.plot(x, val_acces)\n",
        "plt.plot(x, train_losses)\n",
        "plt.plot(x, valid_losses)\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model train and validation loss')"
      ],
      "id": "4K0LikqKs4v3",
      "execution_count": null,
      "outputs": []
    }
  ]
}